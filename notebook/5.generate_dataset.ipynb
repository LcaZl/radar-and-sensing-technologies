{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the parent directory of the current notebook\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../src\"))\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from scripting import (\n",
    "    load_composites, \n",
    "    load_dems,\n",
    "    calculate_aspect_from_dems,\n",
    "    calculate_slope_from_dems,\n",
    "    load_points,\n",
    "    load_lc_points,\n",
    "    load_features,\n",
    "    create_geodf_for_lc_map,\n",
    "    logged_main\n",
    ")\n",
    "\n",
    "from generate_dataset import (\n",
    "    generate_maps,\n",
    "    extract_features_for_points,\n",
    "    enhance_dataset,\n",
    "    inspect_class_distribution,\n",
    "    apply_erosion_and_report,\n",
    "    visual_match_verification_l1,\n",
    "    visual_match_verification_l2,\n",
    "    plot_distance_histogram,\n",
    "    verify_coordinates,\n",
    "    convert_sav_to_csv\n",
    "    )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import dask.distributed\n",
    "from pyproj import CRS\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def read_yaml(file_path: str) -> dict:\n",
    "    \n",
    "    with open(file_path, 'r') as yaml_file: return yaml.safe_load(yaml_file)\n",
    "    \n",
    "def fix_paths_for_nb(input_dict, old_substring = \"/home/hrlcuser/media\", new_substring = \"/media/datapart/lucazanolo\"):\n",
    "    \n",
    "    return {\n",
    "        key: (value.replace(old_substring, new_substring) if isinstance(value, str) else value)\n",
    "        for key, value in input_dict.items()\n",
    "    }\n",
    "    \n",
    "def plot_map(array : np.ndarray, title = \"\"):\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(array, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"Value\")\n",
    "    plt.title(\"Data Visualization\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = fix_paths_for_nb(read_yaml(\"/home/lucazanolo/luca-zanolo/scripts/config_files/5.generate_dataset.yaml\"))\n",
    "points_dataset_name = os.path.basename(parameters['points_dataset_path']).split(\"_\")[-1][:-4]\n",
    "if parameters['dataset_type'] == 'enhanced':\n",
    "    \n",
    "    dataset_id = f\"T{parameters['tile_id']}_f{parameters['features_date']}_{points_dataset_name}_{parameters['enhanced_type']}_{parameters['dataset_type']}\"\n",
    "    \n",
    "    if parameters[\"apply_erosion\"] == True:\n",
    "        dataset_id = f\"{dataset_id}_eroded\"\n",
    "        \n",
    "elif parameters['dataset_type'] == 'fullLC':\n",
    "    \n",
    "    dataset_id = f\"T{parameters['tile_id']}_f{parameters['features_date']}_{points_dataset_name}_{parameters['samples_per_class']}_{parameters['dataset_type']}\"\n",
    "    \n",
    "    if parameters[\"apply_erosion\"] == True:\n",
    "        dataset_id = f\"{dataset_id}_eroded\"\n",
    "        \n",
    "elif parameters['dataset_type'] == 'std':\n",
    "    dataset_id = f\"T{parameters['tile_id']}_f{parameters['features_date']}_{points_dataset_name}_{parameters['dataset_type']}\"\n",
    "\n",
    "dataset_path = f\"{parameters['output_path']}/{dataset_id}.csv\"\n",
    "features_path = f\"{parameters['features_path']}/{parameters['features_date']}/*.tif\"\n",
    "\n",
    "os.makedirs(parameters['report_path'], exist_ok=True)\n",
    "curr_reports_path = f\"{parameters['report_path']}/{dataset_id}_reports\"\n",
    "\n",
    "os.makedirs(curr_reports_path, exist_ok=True)\n",
    "lc_points_map_path = f\"{curr_reports_path}/lcmap_points.html\"\n",
    "points_map_path = f\"{curr_reports_path}/shapefile_points.html\"\n",
    "erosion_report_path = f\"{curr_reports_path}/erosion_report.png\"\n",
    "\n",
    "os.makedirs(parameters['output_path'], exist_ok=True)\n",
    "\n",
    "dataset_path, lc_points_map_path, points_map_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert existing pipeline datasets\n",
    "\n",
    "In \"official\" script if this option is enabled no other datasets will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parameters[\"convert_sav_dataset\"] == True:\n",
    "    dt = convert_sav_to_csv(\n",
    "        sav_path=parameters[\"sav_dataset_path\"],\n",
    "        csv_path=f\"{parameters['output_path']}/{os.path.basename(parameters['sav_dataset_path']).replace('.sav', '.csv')}\"\n",
    "    )\n",
    "\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Composites and DEMs - Compute Aspect and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(os.cpu_count() or 2),\n",
    ") as client:\n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "    \n",
    "    # Load pre computed features\n",
    "\n",
    "    composites = load_composites(parameters[\"composites_path\"], year=parameters[\"composites_year\"], tile=parameters[\"tile_id\"])\n",
    "\n",
    "    dems = load_dems(parameters[\"dems_path\"],year=parameters[\"dems_year\"], tile=parameters[\"tile_id\"])\n",
    "    slope = calculate_slope_from_dems(dems.band_data)\n",
    "    aspect = calculate_aspect_from_dems(dems.band_data)\n",
    "    features = load_features(features_path)\n",
    "    \n",
    "    features_dataset = composites.assign({\n",
    "                        \"dems\":dems.band_data,\n",
    "                        \"slopes\":slope,\n",
    "                        \"aspects\":aspect}).sel(tile=parameters[\"tile_id\"])\n",
    "    \n",
    "    features_dataset = features_dataset.assign({\n",
    "        f_name : feature.isel(time=0) for f_name, feature in features.items() # Only January for GLCM features kept. \n",
    "    })\n",
    "    \n",
    "    print(f\"Dems:\\n{dems}\\n\\n\")\n",
    "    print(f\"Aspect:\\n{aspect}\\n\\n\")\n",
    "    print(f\"Slope:\\n{slope}\\n\\n\")\n",
    "    print(f\"Composites:\\n{composites}\\n\\n\")\n",
    "    print(f\"Features:\\n{features_dataset}\\n\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load points from shape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "composites_crs = CRS.from_wkt(features_dataset.spatial_ref.attrs[\"crs_wkt\"]).to_epsg()\n",
    "points_df, labels_df, tile_geometry = load_points(parameters, features_dataset, composites_crs)\n",
    "lccode2label = {row['LC_code'] : row['description'] for id, row in labels_df.iterrows()}\n",
    "id2lccode = {row['internal_code'] : row['LC_code'] for id, row in labels_df.iterrows()}\n",
    "points_df['class_id'] = points_df['class_id'].map(id2lccode)\n",
    "points_df['class'].value_counts().sort_index().plot(kind='bar', figsize=(10, 6), title=\"Class Distribution in Points Dataset\")\n",
    "\n",
    "points_df, labels_df, tile_geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LC map points and eventually apply erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(os.cpu_count() or 2),\n",
    ") as client:\n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "    if parameters[\"dataset_type\"] != 'std':\n",
    "    \n",
    "        print(\"Loading land cover map points ...\")\n",
    "\n",
    "        lc_map_xr = load_lc_points(parameters)\n",
    "\n",
    "        if parameters[\"apply_erosion\"] == True:\n",
    "            \n",
    "            print(\"Applying erosion ...\")\n",
    "            config = {\n",
    "                'kernel': cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2)), #np.ones((3,3), np.uint8),         # Size of the erosion kernel\n",
    "                'iterations': 1             # Number of iterations for erosion\n",
    "            }\n",
    "\n",
    "            classes_masks = apply_erosion_and_report(lc_map_xr, lccode2label, config, output_path = erosion_report_path)\n",
    "        else:\n",
    "            classes_masks = {id : (lc_map_xr == id).astype(np.uint8) for id in lccode2label.keys() if (lc_map_xr == id).mean() > 0}\n",
    "            \n",
    "        class_counts = points_df['class_id'].value_counts()\n",
    "        max_class_samples = class_counts.max()\n",
    "        avg_class_samples = int(class_counts.mean())\n",
    "        \n",
    "        if parameters[\"dataset_type\"] != 'fullLC':\n",
    "            # Determine target samples per class\n",
    "            if parameters[\"enhanced_type\"] == \"max\":\n",
    "                parameters[\"samples_per_class\"] = max_class_samples\n",
    "            elif parameters[\"enhanced_type\"] == \"avg\":\n",
    "                parameters[\"samples_per_class\"] = avg_class_samples\n",
    "            else:\n",
    "                raise ValueError(\"Invalid value for enhanced_type. Choose 'max' or 'avg'.\")\n",
    "\n",
    "        points_lc_df = create_geodf_for_lc_map(classes_masks, parameters, points_df, features_dataset, lccode2label, composites_crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(os.cpu_count() or 2),\n",
    ") as client:\n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "    print(\"Selecting dataset points ...\")\n",
    "    points_lc_df = None\n",
    "    \n",
    "    if parameters[\"dataset_type\"] != 'std':\n",
    "            \n",
    "        class_counts = points_df['class_id'].value_counts()\n",
    "        max_class_samples = class_counts.max()\n",
    "        avg_class_samples = int(class_counts.mean())\n",
    "        \n",
    "        if parameters[\"dataset_type\"] != 'fullLC':\n",
    "            \n",
    "            # Determine target samples per class\n",
    "            \n",
    "            if parameters[\"enhanced_type\"] == \"max\":\n",
    "                parameters[\"samples_per_class\"] = max_class_samples\n",
    "            elif parameters[\"enhanced_type\"] == \"avg\":\n",
    "                parameters[\"samples_per_class\"] = avg_class_samples\n",
    "            else:\n",
    "                raise ValueError(\"Invalid value for enhanced_type. Choose 'max' or 'avg'.\")\n",
    "\n",
    "        points_lc_df = create_geodf_for_lc_map(classes_masks, parameters, points_df, features_dataset, lccode2label, composites_crs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aligned shapefile points CRS:\\n\", points_df.crs)\n",
    "if points_lc_df is not None:\n",
    "    print(\"Loaded LC map points:\\n\", points_lc_df.crs)\n",
    "    ids_shp = set(points_df['class_id'].unique())\n",
    "    ids_lcmap = set(points_lc_df['class_id'].unique())\n",
    "    print(f\"Classes in shapefile: {ids_shp}\")\n",
    "    print(f\"Classes in lcmap: {ids_lcmap}\")\n",
    "\n",
    "print(\"Features and composites CRS:\\n\", features_dataset.spatial_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate final .csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(os.cpu_count() or 2),\n",
    ") as client:\n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "        \n",
    "    if parameters[\"dataset_type\"] == \"enhanced\":\n",
    "        \n",
    "        ids_shp = set(points_df['class_id'].unique())\n",
    "        ids_lcmap = set(points_lc_df['class_id'].unique())\n",
    "        common_ids = ids_shp & ids_lcmap\n",
    "        points_df = points_df[points_df['class_id'].isin(common_ids)]\n",
    "        points_lc_df = points_lc_df[points_lc_df['class_id'].isin(common_ids)]\n",
    "\n",
    "        dataset = enhance_dataset(points_df, points_lc_df, target_class_col=\"class_id\", samples_per_class=parameters[\"samples_per_class\"])\n",
    "        inspect_class_distribution(dataset, \n",
    "                                   class_column='class', \n",
    "                                   output_path = f\"{curr_reports_path}/classes_distribution.png\",\n",
    "                                   title = f\"Enhanced ({parameters['enhanced_type']}) dataset class distribution - Total points: {len(dataset)}\")\n",
    "\n",
    "    elif parameters[\"dataset_type\"] == \"fullLC\":\n",
    "        \n",
    "        ids_shp = set(points_df['class_id'].unique())\n",
    "        ids_lcmap = set(points_lc_df['class_id'].unique())\n",
    "        common_ids = ids_shp & ids_lcmap\n",
    "        points_df = points_df[points_df['class_id'].isin(common_ids)]\n",
    "        points_lc_df = points_lc_df[points_lc_df['class_id'].isin(common_ids)]\n",
    "\n",
    "        dataset = pd.concat([points_df, points_lc_df[[\"x\",\"y\",\"class_id\",\"class\",\"split\"]]])\n",
    "        inspect_class_distribution(dataset[dataset['split'] == 'train'], \n",
    "                                   class_column='class', \n",
    "                                   output_path = f\"{curr_reports_path}/train_classes_distribution.png\",\n",
    "                                   title = f\"FullLC dataset train split class distribution - Total points: {len(dataset[dataset['split'] == 'train'])}\")\n",
    "        \n",
    "        inspect_class_distribution(dataset[dataset['split'] == 'test'], \n",
    "                                   class_column='class', \n",
    "                                   output_path = f\"{curr_reports_path}/test_classes_distribution.png\",\n",
    "                                   title = f\"FullLC dataset test/val split class distribution - Total points: {len(dataset[dataset['split'] == 'test'])}\")\n",
    "\n",
    "    elif parameters[\"dataset_type\"] == \"std\":\n",
    "        dataset = points_df[[\"x\",\"y\",\"class_id\",\"class\",\"split\"]]\n",
    "        inspect_class_distribution(dataset, \n",
    "                                   class_column='class', \n",
    "                                   output_path = f\"{curr_reports_path}/classes_distribution.png\",\n",
    "                                   title = f\"Standard dataset class distribution - Total points: {len(dataset)}\")\n",
    "\n",
    "    print(\"Generating final dataset ...\")\n",
    "    \n",
    "dt = extract_features_for_points(features_dataset, dataset, dataset_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot UniTN/(UnitTN + PoliMI) points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parameters[\"dataset_type\"] == \"fullLC\" or parameters[\"dataset_type\"] == \"enhanced\":\n",
    "\n",
    "        folium_map1 = generate_maps(points=points_lc_df, \n",
    "                                tile_geometry = tile_geometry,\n",
    "                                        output_file=lc_points_map_path)\n",
    "\n",
    "folium_map2 = generate_maps(points=points_df, \n",
    "                        tile_geometry = tile_geometry,\n",
    "                                output_file=points_map_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_points = verify_coordinates(points_df.copy(), features_dataset)\n",
    "\n",
    "print(verified_points[[\"geometry\", \"x\", \"y\", \"recalc_lon\", \"recalc_lat\", \"diff_lon\", \"diff_lat\"]].head())\n",
    "print(f\"Average longitude difference: {verified_points['diff_lon'].mean()}\")\n",
    "print(f\"Min/Max longitude difference: {verified_points['diff_lon'].min()} - {verified_points['diff_lon'].max()}\")\n",
    "print(f\"Average latitude difference: {verified_points['diff_lat'].mean()}\")\n",
    "print(f\"Min/Max latitude difference: {verified_points['diff_lat'].min()} - {verified_points['diff_lat'].max()}\")\n",
    "\n",
    "plot_distance_histogram(verified_points, save_path=curr_reports_path)\n",
    "visual_match_verification_l1(verified_points, save_path=curr_reports_path)\n",
    "visual_match_verification_l2(verified_points, save_path=curr_reports_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
