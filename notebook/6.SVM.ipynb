{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the parent directory of the current notebook\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../src\"))\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from svm_pipeline import (\n",
    "    train_svm, \n",
    "    test_svm,\n",
    "    multiclass_svm_calibration_curves,\n",
    "    binary_svms_calibration_curves,\n",
    "    create_next_experiment_folder,\n",
    "    preprocess_features,\n",
    "    split_dataset,\n",
    "    features_selection,\n",
    "    SVMS,\n",
    "    SVMS_prediction_test,\n",
    "    save_SVMS, \n",
    "    load_SVMS,\n",
    "    correlation_analysis,\n",
    "    features_importance_analysis,\n",
    "    store_ma_output,\n",
    "    store_rfecv_output,\n",
    "    standardize_dict,\n",
    "    train_binary_svms,\n",
    "    apply_pca,\n",
    "    save_experiment_result,\n",
    "    get_scaler,\n",
    "    grid_search,\n",
    "    reload_and_apply_PCA)\n",
    "\n",
    "from scripting import (\n",
    "    df_numerical_columns_stats, \n",
    "    print_dict, \n",
    "    print_dataframe,\n",
    "    print_list\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from ast import literal_eval\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def read_yaml(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as yaml_file: return yaml.safe_load(yaml_file)\n",
    "    \n",
    "def fix_paths_for_nb(input_dict, old_substring = \"/home/hrlcuser/media\", new_substring = \"/media/datapart/lucazanolo\"):\n",
    "    return {\n",
    "        key: (value.replace(old_substring, new_substring) if isinstance(value, str) else value)\n",
    "        for key, value in input_dict.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = fix_paths_for_nb(read_yaml(\"/home/lucazanolo/luca-zanolo/scripts/config_files/6.svm_pipeline.yaml\"))\n",
    "parameters = {k:v for k,v in parameters.items() if not k.startswith(\"GridSearch\")}\n",
    "\n",
    "os.makedirs(parameters['output_path'], exist_ok=True)\n",
    "experiment_path = create_next_experiment_folder(parameters[\"output_path\"])\n",
    "\n",
    "parameters[\"dataset_type\"] = parameters['dataset_path'].split('_')[-1][:-4]\n",
    "parameters[\"features_inspection_path\"] = f\"{experiment_path}/features_info\"\n",
    "parameters[\"svm_model_path\"] = f\"{experiment_path}/svm.pkl\"\n",
    "\n",
    "parameters[\"calibrated_model_path\"] = f\"{experiment_path}/svm_calibrated.pkl\"\n",
    "parameters[\"calibration_curves_path\"] = f\"{experiment_path}/calibration_curves\"\n",
    "parameters[\"performance_csv_path\"] = f\"{parameters['output_path']}/experiments.csv\"\n",
    "train_class_distribution_path = f\"{parameters['features_inspection_path']}/train_class_distribution.png\"\n",
    "test_class_distribution_path = f\"{parameters['features_inspection_path']}/test_class_distribution.png\"\n",
    "cal_class_distribution_path = f\"{parameters['features_inspection_path']}/cal_class_distribution.png\"\n",
    "\n",
    "os.makedirs(parameters[\"features_inspection_path\"], exist_ok=True)\n",
    "os.makedirs(parameters[\"calibration_curves_path\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(parameters[\"dataset_path\"])\n",
    "dataset[\"ground_truth_index\"].astype(int)\n",
    "parameters[\"non_features_columns\"] = [f for f in parameters[\"non_features_columns\"] if f in dataset.columns]\n",
    "\n",
    "df_numerical_columns_stats(dataset, title = f\"\\nFull dataset samples: {len(dataset)}\\nColumns info:\")\n",
    "print_dataframe(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test, dataset_calibration = split_dataset(dataset, parameters, generate_report=True)\n",
    "dataset_train, preprocessing_metadata = preprocess_features(dataset_train, parameters)\n",
    "\n",
    "scalers = {f : get_scaler(info[\"method\"], info[\"params\"]) for f, info in preprocessing_metadata.items()}\n",
    "for feature_name in dataset_train.keys():\n",
    "    \n",
    "    if feature_name in parameters[\"non_features_columns\"] or feature_name in parameters[\"exluded_from_preprocessing\"]:\n",
    "        continue\n",
    "    \n",
    "    dataset_test[feature_name] = scalers[feature_name].transform(dataset_test[feature_name].values.reshape(-1,1)).flatten()\n",
    "    dataset_calibration[feature_name] = scalers[feature_name].transform(dataset_calibration[feature_name].values.reshape(-1,1)).flatten()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters[\"verbose\"] = True\n",
    "    \n",
    "features_to_remove, features_to_use, selection_report = features_selection(parameters, dataset_train)\n",
    "\n",
    "assert features_to_use.intersection(features_to_remove) == set(),  \"Detected a feature to be removed and used at the same time.\"\n",
    "assert len(features_to_remove) + len(features_to_use) == len(dataset_train.columns),  f\"Detected inconsistent number of dataset columns in features to use and to remove.\\nFeatures to use: {len(features_to_use)}\\nFeatures to remove: {len(features_to_remove)}\"\n",
    "\n",
    "print(f\"\\nLV. 0 - Total features to analyze: {len(dataset_train.columns)} ({len(parameters['non_features_columns'])} Non Features)\")\n",
    "print(f\"LV. 1 - Features intially removed: {len(selection_report['removed_features_lv1'])}\")\n",
    "print(f\"LV. 2 - Removed constant Features: {len(selection_report['constant_features'])}\")\n",
    "print(f\"LV. 3 - Features removed with {parameters['features_selection_strategy']} selection strategy: {len(selection_report['removed_features_lv2'])}\")\n",
    "print(f\"LV. 4 - Features to be used: {len(features_to_use)} ({len(parameters['non_features_columns'])} Non Features)\")\n",
    "\n",
    "if len(features_to_use) == len(parameters['non_features_columns']):\n",
    "    raise ValueError(\"Parameters configuration error. Specify at least a feature to use.\")\n",
    "\n",
    "\n",
    "if selection_report[\"type\"] == \"multicollinearity_analysis\" and parameters[\"verbose\"] == True:\n",
    "    \n",
    "    correlation_matrix = selection_report[\"info\"][\"correlation_matrix\"]\n",
    "    features_importances = selection_report[\"info\"][\"features_importance\"]\n",
    "    store_ma_output(correlation_matrix, features_importances, parameters, note = \"pre_selection\")\n",
    "\n",
    "elif selection_report[\"type\"] == \"rfecv\" and parameters[\"verbose\"] == True:\n",
    "    \n",
    "    correlation_matrix = correlation_analysis(dataset_train, parameters)\n",
    "    features_importances = features_importance_analysis(dataset_train, parameters, target_col='ground_truth_index')\n",
    "    store_ma_output(correlation_matrix, features_importances, parameters, note = \"pre_selection\")\n",
    "\n",
    "    store_rfecv_output(selection_report, parameters, note=\"\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    correlation_matrix = correlation_analysis(dataset_train, parameters)\n",
    "    features_importances = features_importance_analysis(dataset_train, parameters, target_col='ground_truth_index')\n",
    "    store_ma_output(correlation_matrix, features_importances, parameters, note = \"pre_selection\")\n",
    "\n",
    "ordered_cols = [col for col in dataset.columns if col in features_to_use]\n",
    "dataset_train = dataset_train[ordered_cols]\n",
    "dataset_calibration = dataset_calibration[list(dataset_train.keys())]\n",
    "dataset_test = dataset_test[list(dataset_train.keys())]\n",
    "\n",
    "correlation_matrix = correlation_analysis(dataset_train, parameters)\n",
    "features_importances = features_importance_analysis(dataset_train, parameters, target_col='ground_truth_index')\n",
    "store_ma_output(correlation_matrix, features_importances, parameters, note = \"post_selection\")\n",
    "\n",
    "parameters[\"features_not_used\"] = features_to_remove\n",
    "features_selected = dataset_train.columns.difference(parameters[\"non_features_columns\"])\n",
    "    \n",
    "assert not dataset_train.isnull().any().any(), \"NaN found in training set\"\n",
    "assert not dataset_test.isnull().any().any(), \"NaN found in test set\"\n",
    "assert not dataset_calibration.isnull().any().any(), \"NaN found in calibration set\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = None\n",
    "if parameters[\"apply_pca\"] == True:\n",
    "            \n",
    "    dataset_train, pca = apply_pca(dataset_train, parameters)\n",
    "    \n",
    "    correlation_matrix = correlation_analysis(dataset_train, parameters)\n",
    "    features_importances = features_importance_analysis(dataset_train, parameters, target_col='ground_truth_index')\n",
    "    store_ma_output(correlation_matrix, features_importances, parameters, note = \"post_pca\")\n",
    "    \n",
    "    dataset_test = reload_and_apply_PCA(pca, dataset_test, parameters)\n",
    "    dataset_calibration = reload_and_apply_PCA(pca, dataset_calibration, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Single SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.linspace(100, 1000, 5),\n",
    "    'gamma': np.linspace(1e-3, 10, 10),\n",
    "    'probability' : [True],\n",
    "}\n",
    "\n",
    "Y_train = dataset_train[\"ground_truth_label\"]\n",
    "Y_test = dataset_test[\"ground_truth_label\"]\n",
    "Y_cal = dataset_calibration[\"ground_truth_label\"]\n",
    "\n",
    "X_train = dataset_train.drop(columns=parameters[\"non_features_columns\"])\n",
    "X_test = dataset_test.drop(columns=parameters[\"non_features_columns\"])[X_train.columns]\n",
    "X_cal = dataset_calibration.drop(columns=parameters[\"non_features_columns\"])[X_train.columns]\n",
    "\n",
    "features_used = list(X_train.columns.difference(parameters[\"non_features_columns\"]))\n",
    "    \n",
    "classes = np.unique(Y_train)\n",
    "\n",
    "svm_grid_search = train_svm(X_train, Y_train, param_grid, cv=3)\n",
    "\n",
    "svm = svm_grid_search.best_estimator_\n",
    "best_params = svm_grid_search.best_params_\n",
    "best_params_std = standardize_dict(best_params, \"SVM\")\n",
    "print_dict(best_params, f\"Best SVM parameters\")\n",
    "\n",
    "svm_test_report = test_svm(svm, X_test, Y_test, verbose=True)\n",
    "svm_test_report = standardize_dict(svm_test_report, \"EVAL\")\n",
    "svm_test_report.update(best_params_std)\n",
    "\n",
    "svm_calibrated = CalibratedClassifierCV(estimator=svm, method='isotonic', cv='prefit', n_jobs=-1)\n",
    "svm_calibrated.fit(X_cal, Y_cal)\n",
    "\n",
    "svm_calibration_report = test_svm(svm_calibrated, X_test, Y_test, verbose=True)\n",
    "svm_calibration_report = standardize_dict(svm_calibration_report, \"EVALCAL\")\n",
    "svm_test_report.update(svm_calibration_report)\n",
    "mcsvm_report = svm_test_report.copy()\n",
    "\n",
    "report_df = pd.DataFrame([mcsvm_report])\n",
    "print_dataframe(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Binary SVMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': np.linspace(100, 1000, 5), \n",
    "    'gamma': np.linspace(1e-3, 10, 10), \n",
    "    'probability' : [False]\n",
    "}\n",
    "print(\"Training binary SVMs with parameters grid:\")\n",
    "print(param_grid)\n",
    "svms = train_binary_svms(parameters, param_grid, X_train, X_test[X_train.columns], X_cal[X_train.columns], Y_train, Y_test, Y_cal)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SVMs class and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(Y_train)\n",
    "\n",
    "model = SVMS(svm=svm,\n",
    "             svm_calibrated=svm_calibrated,\n",
    "             svms=svms, \n",
    "             parameters=parameters, \n",
    "             classes=classes, \n",
    "             preprocessing_metadata=preprocessing_metadata, \n",
    "             training_report= mcsvm_report,  \n",
    "             pca=pca, \n",
    "             features_selected = features_selected,\n",
    "             features_used = features_used)\n",
    "\n",
    "save_SVMS(model, parameters['svm_model_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_svm_calibration_curves(model, X_cal, Y_cal, output_dir=parameters['calibration_curves_path'], show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_svms_calibration_curves(model, X_cal, Y_cal, parameters['calibration_curves_path'], show_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM UNcalibrated prediction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_binary_svms = False\n",
    "model.use_multiclass_svm_calibrated = False\n",
    "model.use_binary_svms_with_softmax = False\n",
    "SVMS_prediction_test(model, X_test, Y_test, n_samples=None, chart_title=\"Uncalibrated Multiclass SVM - Distribution of predicted labels\")\n",
    "print(f\"SVM evaluation on test set:\\n\\n\")\n",
    "model.verbose = False\n",
    "_ = test_svm(model, X_test, Y_test, verbose=True)\n",
    "# Plot the calibration curves of the single MultiClass sklearn SVM (If has probability = True) and the same SVM calibrated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM calibrated prediction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_binary_svms = False\n",
    "model.use_multiclass_svm_calibrated = True\n",
    "model.use_binary_svms_with_softmax = False\n",
    "\n",
    "SVMS_prediction_test(model, X_test, Y_test, n_samples=None, chart_title=\"Calibrated Multiclass SVM - Distribution of predicted labels\")\n",
    "print(f\"SVM evaluation on test set:\\n\\n\")\n",
    "model.verbose = False\n",
    "_ = test_svm(model, X_test, Y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary SVMs prediction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_binary_svms = True\n",
    "model.use_multiclass_svm_calibrated = False\n",
    "model.use_binary_svms_with_softmax = False\n",
    "SVMS_prediction_test(model, X_test, Y_test, n_samples=None)\n",
    "print(f\"SVM evaluation on test set:\\n\\n\")\n",
    "model.verbose = False\n",
    "_ = test_svm(model, X_test, Y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary SVM with softmax prediction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_binary_svms = True\n",
    "model.use_multiclass_svm_calibrated = False\n",
    "model.use_binary_svms_with_softmax = True\n",
    "SVMS_prediction_test(model, X_test, Y_test, n_samples=None)\n",
    "print(f\"SVM evaluation on test set:\\n\\n\")\n",
    "model.verbose = False\n",
    "_ = test_svm(model, X_test, Y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved model and display info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_display_svms(filepath):\n",
    "    \"\"\"Loads an SVMS model from a .pkl file and prints its details in a structured format.\"\"\"\n",
    "    \n",
    "    svms_model = joblib.load(filepath)\n",
    "\n",
    "    # Extract experiment parameters\n",
    "    experiment_params = svms_model.experiment_parameters\n",
    "\n",
    "    # Extract class-related information\n",
    "    class_info = {\n",
    "        \"Number of Classes\": svms_model.n_classes,\n",
    "        \"Classes\": svms_model.classes,\n",
    "        \"ID to Class Mapping\": svms_model.id2class,\n",
    "        \"Class to ID Mapping\": svms_model.class2id\n",
    "    }\n",
    "\n",
    "    class_info_cleaned = {\n",
    "        k: str(v.tolist()) if isinstance(v, np.ndarray) else str(v) if isinstance(v, (list, dict)) else v \n",
    "        for k, v in class_info.items()\n",
    "    }\n",
    "\n",
    "    # Extract multiclass SVM details\n",
    "    multiclass_info = {\n",
    "        \"Multiclass SVM Type\": type(svms_model.multiclass_svm).__name__,\n",
    "        \"Use Calibrated Multiclass SVM\": svms_model.use_multiclass_svm_calibrated\n",
    "    }\n",
    "\n",
    "    # Extract binary SVMs information\n",
    "    binary_svm_info = {\n",
    "        \"Use Binary SVMs\": svms_model.use_binary_svms,\n",
    "        \"Use Softmax with Binary SVMs\": svms_model.use_binary_svms_with_softmax,\n",
    "        \"Number of Binary SVMs\": len(svms_model.binary_svms),\n",
    "        \"Binary SVM Classes\": str(list(svms_model.binary_svms.keys()))  # Convert list to string\n",
    "    }\n",
    "\n",
    "    # Extract preprocessing and PCA metadata\n",
    "    preprocessing_metadata = svms_model.preprocessing_metadata\n",
    "    pca = svms_model.pca if svms_model.pca else \"Not Used\"\n",
    "\n",
    "    # Extract binary SVM reports\n",
    "    binary_svm_reports = svms_model.binary_svms_reports\n",
    "\n",
    "    # Print structured output\n",
    "    print(\"\\n===== SVMS Model Information =====\")\n",
    "    \n",
    "    print(\"\\n--- Experiment Parameters ---\")\n",
    "    print_dataframe(pd.DataFrame(experiment_params.items(), columns=[\"Parameter\", \"Value\"]), title=\"Experiment Parameters\")\n",
    "\n",
    "    print(\"\\n--- Class Information ---\")\n",
    "    print_dataframe(pd.DataFrame(class_info_cleaned.items(), columns=[\"Attribute\", \"Value\"]), title=\"Class Information\")\n",
    "\n",
    "    print(\"\\n--- Multiclass SVM Information ---\")\n",
    "    print_dataframe(pd.DataFrame(multiclass_info.items(), columns=[\"Attribute\", \"Value\"]), title=\"Multiclass SVM\")\n",
    "\n",
    "    print(\"\\n--- Binary SVM Information ---\")\n",
    "    print_dataframe(pd.DataFrame(binary_svm_info.items(), columns=[\"Attribute\", \"Value\"]), title=\"Binary SVM Information\")\n",
    "\n",
    "    print(\"\\n--- Preprocessing Metadata ---\")\n",
    "    print_dataframe(pd.DataFrame(preprocessing_metadata.items(), columns=[\"Step\", \"Details\"]), title=\"Preprocessing Metadata\")\n",
    "\n",
    "    print(\"\\n--- PCA Metadata ---\")\n",
    "    print_dataframe(pd.DataFrame({\"PCA\": [pca]}), title=\"PCA Metadata\")\n",
    "\n",
    "    print(\"\\n--- Binary SVM Performance Reports ---\")\n",
    "    for cls, report in binary_svm_reports.items():\n",
    "        print(f\"\\nBinary SVM Report for Class: {cls}\")\n",
    "        print_dataframe(pd.DataFrame(report).T, title=f\"Performance Report for {cls}\")\n",
    "\n",
    "svms_model_path = \"/media/datapart/lucazanolo/SVM/lc_maps/best_svm/svm.pkl\"\n",
    "load_and_display_svms(svms_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = fix_paths_for_nb(read_yaml(\"/home/lucazanolo/luca-zanolo/scripts/config_files/6.svm_pipeline.yaml\"))\n",
    "\n",
    "grid_search(parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/media/datapart/lucazanolo/SVM/evaluation/experiments/grid_search.csv')\n",
    "print_dataframe(df[['SVMS_AVGF1','SVM_AVGF1']].sort_values(by=['SVMS_AVGF1'], ascending=False), limit=3)\n",
    "print_dataframe(df[['SVMS_AVGF1','SVM_AVGF1']].sort_values(by=['SVM_AVGF1'], ascending=False), limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_avg_f1_structured(df, fields, score_col=\"SVM_AVGF1\", output_path=\"avg_f1_structured_fixed.png\", aggregate_fn=\"mean\", title=\"\", top_k=20):\n",
    "    if aggregate_fn not in [\"mean\", \"max\"]:\n",
    "        raise ValueError(\"aggregate_fn must be either 'mean' or 'max'\")\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for field in fields:\n",
    "        if field not in df.columns:\n",
    "            print(f\"Skipping '{field}' — not in DataFrame.\")\n",
    "            continue\n",
    "\n",
    "        if aggregate_fn == \"mean\" and top_k is not None:\n",
    "            grouped_scores = (\n",
    "                df[[field, score_col]]\n",
    "                .dropna(subset=[score_col])\n",
    "                .groupby(field, dropna=False)\n",
    "                .apply(lambda g: g.nlargest(top_k, score_col)[score_col].mean())\n",
    "                .reset_index(name=score_col)\n",
    "            )\n",
    "        else:\n",
    "            grouped_scores = getattr(df.groupby(field, dropna=False)[score_col], aggregate_fn)().reset_index()\n",
    "\n",
    "        for _, row in grouped_scores.iterrows():\n",
    "            val = row[field]\n",
    "            val_str = \"NaN\" if pd.isna(val) else str(val)\n",
    "            records.append({\n",
    "                \"field\": field,\n",
    "                \"value\": val_str,\n",
    "                \"score\": row[score_col]\n",
    "            })\n",
    "\n",
    "    if not records:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "\n",
    "    records.sort(key=lambda r: (r[\"field\"], r[\"value\"]))\n",
    "    max_field_len = max(len(r[\"field\"]) for r in records)\n",
    "\n",
    "    plot_data = []\n",
    "    label_styles = []\n",
    "\n",
    "    for i, field in enumerate(fields):\n",
    "        if i != 0:\n",
    "            plot_data.append((\" \", None))\n",
    "            label_styles.append(\"spacer\")\n",
    "\n",
    "        header_indent = ' ' * (max_field_len // 2)\n",
    "        plot_data.append((f\" {field}\", None))\n",
    "        label_styles.append(\"header\")\n",
    "\n",
    "        for r in [r for r in records if r[\"field\"] == field]:\n",
    "            label = f\"  {r['value']:<25} {(r['score'] * 100):.3f}%\"  # Nome + punteggio nella stessa label\n",
    "            plot_data.append((label, r[\"score\"]))\n",
    "            label_styles.append(\"value\")\n",
    "\n",
    "\n",
    "    if plot_data[-1][1] is None and label_styles[-1] == \"spacer\":\n",
    "        plot_data.pop()\n",
    "        label_styles.pop()\n",
    "\n",
    "    labels = [label for label, _ in plot_data]\n",
    "    scores = [score for _, score in plot_data]\n",
    "    y_pos = list(range(len(labels)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, max(6, len(labels) * 0.35)))\n",
    "    bar_values = [s if s is not None else 0 for s in scores]\n",
    "    bar_colors = [\"cornflowerblue\" if style == \"value\" else \"white\" for style in label_styles]\n",
    "\n",
    "    bars = ax.barh(y_pos, bar_values, color=bar_colors)\n",
    "    ax.set_xlim(0, 1) \n",
    "\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([\"\"] * len(labels))\n",
    "\n",
    "    for i, (label, style) in enumerate(zip(labels, label_styles)):\n",
    "        if style == \"header\":\n",
    "            ax.text(0, i, label, va=\"center\", ha=\"left\", fontsize=10, fontweight=\"bold\", family=\"monospace\")\n",
    "        elif style == \"value\":\n",
    "            ax.text(0, i, label, va=\"center\", ha=\"left\", fontsize=9, family=\"monospace\")\n",
    "\n",
    "    ax.set_xlabel(f\"{aggregate_fn.capitalize()} {score_col}\")\n",
    "    ax.set_title(f\"Top-{top_k} {title}\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {output_path}\")\n",
    "\n",
    "\n",
    "df[\"dataset_name\"] = df[\"dataset_path\"].apply(lambda p: os.path.basename(str(p))[:-4] if pd.notna(p) else p)\n",
    "fields_to_check = [\n",
    "    \"dataset_type\",\n",
    "    \"dataset_name\",\n",
    "    \"use_glcm_features\",\n",
    "    \"use_other_features\",\n",
    "    \"band_features_preprocessing\",\n",
    "    \"other_features_preprocessing\",\n",
    "    \"features_selection_strategy\",\n",
    "    \"fss_use_bands\",\n",
    "    \"apply_pca\",\n",
    "    \"pca_use_bands\"\n",
    "]\n",
    "\n",
    "\n",
    "plot_avg_f1_structured(\n",
    "    df,\n",
    "    fields=fields_to_check,\n",
    "    score_col=\"SVM_AVGF1\",\n",
    "    output_path=\"svm_avg_f1_structured_fixed.png\",\n",
    "    aggregate_fn='mean',\n",
    "    title = \"Multiclass SVM Grid Search average results\",\n",
    "    #title = \"Binary SVMs Grid Search average results\",\n",
    "    top_k=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"/media/datapart/lucazanolo/SVM/evaluation/experiments/grid_search.csv\")\n",
    "best_svm_idx = df[\"SVM_AVGF1\"].idxmax()\n",
    "best_svm = df.loc[best_svm_idx]\n",
    "\n",
    "svm_eval_metrics = best_svm.filter(like=\"SVM_EVAL_\")\n",
    "svm_evalcal_metrics = best_svm.filter(like=\"SVM_EVALCAL_\")\n",
    "\n",
    "svm_params = best_svm[\n",
    "    [\n",
    "        \"SVM_params_C\",\n",
    "        \"SVM_params_gamma\",\n",
    "        \"SVM_params_probability\",\n",
    "        \"apply_pca\",\n",
    "        \"pca_use_bands\",\n",
    "        \"band_features_preprocessing\",\n",
    "        \"dataset_path\",\n",
    "        \"dataset_type\",\n",
    "        \"drop_constant_features\",\n",
    "        \"features_selection_strategy\",\n",
    "        \"fss_use_bands\",\n",
    "        \"id\",\n",
    "        \"log_name\",\n",
    "        \"ma_correlation_threshold\",\n",
    "        \"non_features_columns\",\n",
    "        \"other_features_preprocessing\",\n",
    "        \"use_band_features\",\n",
    "        \"use_glcm_features\",\n",
    "        \"use_other_features\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Function to format classification report\n",
    "def format_classification_report(metrics, prefix):\n",
    "    class_metrics = {}\n",
    "\n",
    "    for col in metrics.index:\n",
    "        parts = col.replace(prefix, \"\").split(\"_\")\n",
    "        if len(parts) >= 2:\n",
    "            class_label = parts[0]\n",
    "            metric_type = \"_\".join(parts[1:])\n",
    "\n",
    "            if class_label not in class_metrics:\n",
    "                class_metrics[class_label] = {}\n",
    "\n",
    "            class_metrics[class_label][metric_type] = metrics[col]\n",
    "\n",
    "    report_data = []\n",
    "    for cls, metrics_dict in sorted(class_metrics.items()):\n",
    "        if all(key in metrics_dict for key in [\"precision\", \"recall\", \"f1-score\", \"support\"]):\n",
    "            report_data.append([\n",
    "                cls,\n",
    "                metrics_dict[\"precision\"],\n",
    "                metrics_dict[\"recall\"],\n",
    "                metrics_dict[\"f1-score\"],\n",
    "                metrics_dict[\"support\"],\n",
    "            ])\n",
    "\n",
    "    report_df = pd.DataFrame(report_data, columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n",
    "    \n",
    "    return report_df\n",
    "\n",
    "eval_report = format_classification_report(svm_eval_metrics, prefix=\"SVM_EVAL_\")\n",
    "evalcal_report = format_classification_report(svm_evalcal_metrics, prefix=\"SVM_EVALCAL_\")\n",
    "\n",
    "print(\"\\n===== Best Multi-Class SVM Performance =====\")\n",
    "print(f\"SVM_AVGF1: {best_svm['SVM_AVGF1']}\")\n",
    "\n",
    "print(\"\\n--- Uncalibrated SVM Performance (EVAL) ---\")\n",
    "print_dataframe(eval_report, title=\"Uncalibrated SVM Report\")\n",
    "\n",
    "print(\"\\n--- Calibrated SVM Performance (EVALCAL) ---\")\n",
    "print_dataframe(evalcal_report, title=\"Calibrated SVM Report\")\n",
    "\n",
    "print(\"\\n===== Best Multi-Class SVM Configuration Parameters =====\")\n",
    "print_dataframe(pd.DataFrame(svm_params), title=\"Best multiclass SVM Configuration Parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/media/datapart/lucazanolo/SVM/evaluation/experiments/grid_search.csv\")\n",
    "best_svms_idx = df[\"SVMS_AVGF1\"].idxmax()\n",
    "best_svms = df.loc[best_svms_idx]\n",
    "\n",
    "svms_eval_metrics = best_svms.filter(like=\"SVMS_EVAL_\")\n",
    "svms_evalcal_metrics = best_svms.filter(like=\"SVMS_EVALCAL_\")\n",
    "\n",
    "svms_params = best_svms[\n",
    "    [\n",
    "        \"SVM_params_C\",\n",
    "        \"SVM_params_gamma\",\n",
    "        \"SVM_params_probability\",\n",
    "        \"apply_pca\",\n",
    "        \"pca_use_bands\",\n",
    "        \"band_features_preprocessing\",\n",
    "        \"dataset_path\",\n",
    "        \"dataset_type\",\n",
    "        \"drop_constant_features\",\n",
    "        \"features_selection_strategy\",\n",
    "        \"fss_use_bands\",\n",
    "        \"id\",\n",
    "        \"log_name\",\n",
    "        \"ma_correlation_threshold\",\n",
    "        \"non_features_columns\",\n",
    "        \"other_features_preprocessing\",\n",
    "        \"use_band_features\",\n",
    "        \"use_glcm_features\",\n",
    "        \"use_other_features\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Function to extract class macro metrics\n",
    "def extract_class_macro_metrics(eval_metrics, evalcal_metrics):\n",
    "    class_macro_metrics = {}\n",
    "\n",
    "    for col in eval_metrics.index:\n",
    "        match = re.match(r\"SVMS_EVAL_([\\w\\-]+)_macro-avg_(f1-score|precision|recall|support)\", col)\n",
    "        if match:\n",
    "            class_name, metric_type = match.groups()\n",
    "\n",
    "            if class_name not in class_macro_metrics:\n",
    "                class_macro_metrics[class_name] = {}\n",
    "\n",
    "            class_macro_metrics[class_name][f\"{metric_type}_eval\"] = eval_metrics[col]\n",
    "            class_macro_metrics[class_name][f\"{metric_type}_evalcal\"] = evalcal_metrics.get(\n",
    "                col.replace(\"SVMS_EVAL_\", \"SVMS_EVALCAL_\"), None\n",
    "            )\n",
    "\n",
    "    report_data = []\n",
    "    for cls, metrics_dict in sorted(class_macro_metrics.items()):\n",
    "        if all(key in metrics_dict for key in [\"f1-score_eval\", \"precision_eval\", \"recall_eval\"]):\n",
    "            report_data.append([\n",
    "                cls,\n",
    "                metrics_dict[\"precision_eval\"],\n",
    "                metrics_dict[\"recall_eval\"],\n",
    "                metrics_dict[\"f1-score_eval\"],\n",
    "                metrics_dict[\"support_eval\"],\n",
    "                metrics_dict[\"precision_evalcal\"],\n",
    "                metrics_dict[\"recall_evalcal\"],\n",
    "                metrics_dict[\"f1-score_evalcal\"],\n",
    "                metrics_dict[\"support_evalcal\"],\n",
    "            ])\n",
    "\n",
    "    report_df = pd.DataFrame(report_data, columns=[\n",
    "        \"Class\", \n",
    "        \"Precision (EVAL)\", \"Recall (EVAL)\", \"F1-Score (EVAL)\", \"Support (EVAL)\",\n",
    "        \"Precision (EVALCAL)\", \"Recall (EVALCAL)\", \"F1-Score (EVALCAL)\", \"Support (EVALCAL)\"\n",
    "    ])\n",
    "    \n",
    "    return report_df\n",
    "\n",
    "svms_macro_report = extract_class_macro_metrics(svms_eval_metrics, svms_evalcal_metrics)\n",
    "\n",
    "print(\"\\n===== Best Binary SVM Performance =====\")\n",
    "print(f\"SVMS_AVGF1: {best_svms['SVMS_AVGF1']}\")\n",
    "\n",
    "print(\"\\n--- Binary SVM Macro Metrics (Before & After Calibration) ---\")\n",
    "print_dataframe(svms_macro_report, title=\"Binary SVM Macro Metrics (Before & After Calibration)\")\n",
    "\n",
    "print(\"\\n===== Best Binary SVM Configuration Parameters =====\")\n",
    "print_dataframe(pd.DataFrame(svms_params), title=\"Best Binary SVM Configuration Parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .CSV writing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Assume save_experiment_result and handle_value are already defined above\n",
    "\n",
    "# Test CSV path\n",
    "test_csv_path = \"test_experiment_results.csv\"\n",
    "\n",
    "# Remove old file if it exists\n",
    "if os.path.exists(test_csv_path):\n",
    "    os.remove(test_csv_path)\n",
    "\n",
    "# Test case 1: Initial write with basic keys\n",
    "data1 = {\n",
    "    'accuracy': 0.95,\n",
    "    'loss': 0.1,\n",
    "    'labels': ['cat', 'dog']\n",
    "}\n",
    "save_experiment_result(data1, test_csv_path)\n",
    "\n",
    "# Test case 2: Add new data with a new key 'f1_score'\n",
    "data2 = {\n",
    "    'accuracy': 0.92,\n",
    "    'loss': 0.15,\n",
    "    'f1_score': 0.89\n",
    "}\n",
    "save_experiment_result(data2, test_csv_path)\n",
    "\n",
    "# Test case 3: Add data missing 'loss', but with new key 'precision'\n",
    "data3 = {\n",
    "    'accuracy': 0.90,\n",
    "    'precision': 0.87\n",
    "}\n",
    "save_experiment_result(data3, test_csv_path)\n",
    "\n",
    "# Test case 4: Add data with all fields known so far\n",
    "data4 = {\n",
    "    'accuracy': 0.94,\n",
    "    'loss': 0.12,\n",
    "    'f1_score': 0.91,\n",
    "    'precision': 0.88,\n",
    "    'labels': ['horse']\n",
    "}\n",
    "save_experiment_result(data4, test_csv_path)\n",
    "\n",
    "# Read back and print the CSV to verify\n",
    "with open(test_csv_path, 'r') as f:\n",
    "    print(\"\\n--- Final CSV Content ---\")\n",
    "    for line in f:\n",
    "        print(line.strip())\n",
    "\n",
    "# 1. Basic normal row\n",
    "save_experiment_result({\n",
    "    'model': 'resnet',\n",
    "    'accuracy': 0.91,\n",
    "    'params': [128, 64, 32]\n",
    "}, test_csv_path)\n",
    "\n",
    "# 2. Special characters and strings with commas/newlines/quotes\n",
    "save_experiment_result({\n",
    "    'model': 'bert,base\\n\"quoted\"',\n",
    "    'accuracy': 0.88,\n",
    "    'notes': 'Used batch_size=32, lr=0.001'\n",
    "}, test_csv_path)\n",
    "\n",
    "# 3. Missing key 'accuracy', added new key 'f1_score'\n",
    "save_experiment_result({\n",
    "    'model': 'efficientnet',\n",
    "    'f1_score': 0.84\n",
    "}, test_csv_path)\n",
    "\n",
    "# 4. All fields missing except 'model'\n",
    "save_experiment_result({\n",
    "    'model': 'squeezenet'\n",
    "}, test_csv_path)\n",
    "\n",
    "# 5. Extra keys not seen before\n",
    "save_experiment_result({\n",
    "    'model': 'vgg16',\n",
    "    'accuracy': 0.82,\n",
    "    'precision': 0.80,\n",
    "    'recall': 0.78,\n",
    "    'complexity': np.array([10, 20, 30])\n",
    "}, test_csv_path)\n",
    "\n",
    "# 6. Empty dict (just test handling)\n",
    "save_experiment_result({}, test_csv_path)\n",
    "\n",
    "# 7. Key ordering test (should not break alignment)\n",
    "save_experiment_result({\n",
    "    'params': [32, 64],\n",
    "    'accuracy': 0.90,\n",
    "    'model': 'mlp'\n",
    "}, test_csv_path)\n",
    "\n",
    "# 8. Numpy types and set\n",
    "save_experiment_result({\n",
    "    'model': 'rnn',\n",
    "    'accuracy': np.float64(0.85),\n",
    "    'layers': {1, 2, 3}\n",
    "}, test_csv_path)\n",
    "\n",
    "# 9. None values explicitly\n",
    "save_experiment_result({\n",
    "    'model': None,\n",
    "    'accuracy': None,\n",
    "    'notes': None\n",
    "}, test_csv_path)\n",
    "\n",
    "# 10. Simulated high volume (light)\n",
    "for i in range(5):\n",
    "    save_experiment_result({\n",
    "        'model': f'random_model_{i}',\n",
    "        'accuracy': round(0.75 + 0.01 * i, 3)\n",
    "    }, test_csv_path)\n",
    "\n",
    "# Read and print\n",
    "with open(test_csv_path, 'r') as f:\n",
    "    print(\"\\n--- Stress Test Output ---\")\n",
    "    for line in f:\n",
    "        print(line.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
