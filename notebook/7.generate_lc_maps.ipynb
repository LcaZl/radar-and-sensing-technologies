{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the parent directory of the current notebook\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../src\"))\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from scripting import load_dems, calculate_aspect_from_dems, calculate_slope_from_dems, load_composites, print_dataframe\n",
    "from scripting import load_features\n",
    "from svm_pipeline import SVMS, load_SVMS, DegreeToSinCos\n",
    "from generate_lc_map import expand_bands_and_reduce, create_empty_tif,process_chunk, load_map_and_plot, load_map_with_probs_and_plot, map_lc_codes_to_rgba\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.transform import from_origin\n",
    "from dask import delayed\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from svm_pipeline import get_scaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import copy\n",
    "import time\n",
    "\n",
    "def read_yaml(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as yaml_file: return yaml.safe_load(yaml_file)\n",
    "\n",
    "def fix_paths_for_nb(input_dict, old_substring = \"/home/hrlcuser/media\", new_substring = \"/media/datapart/lucazanolo\"):\n",
    "    return {\n",
    "        key: (value.replace(old_substring, new_substring) if isinstance(value, str) else value)\n",
    "        for key, value in input_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = fix_paths_for_nb(read_yaml(\"/home/lucazanolo/luca-zanolo/scripts/config_files/7.generate_lc_map.yaml\"))\n",
    "os.makedirs(parameters[\"output_path\"], exist_ok=True)\n",
    "parameters[\"classified_image_path\"] = f\"{parameters['output_path']}/{parameters['tile_id']}_{parameters['composites_year']}_{parameters['chunks_limit']}r\"\n",
    "png_images_path = f\"{parameters['output_path']}/images\"\n",
    "os.makedirs(png_images_path, exist_ok=True)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(os.cpu_count() or 2),\n",
    ") as client:\n",
    "    \n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "    print(\"Loading dataset.\")\n",
    "\n",
    "    glcm_year, glcm_month = str(parameters['features_path'].split('/')[-2]).split(\"-\")\n",
    "\n",
    "    composites = load_composites(parameters[\"composites_path\"], year=parameters[\"composites_year\"], tile=parameters[\"tile_id\"])\n",
    "    dems = load_dems(parameters[\"dems_path\"] ,year=parameters[\"dems_year\"], tile=parameters[\"tile_id\"])\n",
    "    slope = calculate_slope_from_dems(dems.band_data)\n",
    "    aspect = calculate_aspect_from_dems(dems.band_data)\n",
    "    glcm_features = load_features(parameters['features_path'])\n",
    "\n",
    "    dataset = composites.assign({\n",
    "                        \"dems\":dems.band_data,\n",
    "                        \"slopes\":slope,\n",
    "                        \"aspects\":aspect}).sel(tile=parameters[\"tile_id\"])\n",
    "    \n",
    "    dataset = dataset.assign({\n",
    "        f_name : feature.isel(time=0) for f_name, feature in glcm_features.items() # Only January GLCM features are kept. \n",
    "    })\n",
    "    \n",
    "    \n",
    "    print(f\"Dems:\\n{dems}\\n\\n\")\n",
    "    print(f\"Aspect:\\n{aspect}\\n\\n\")\n",
    "    print(f\"Slope:\\n{slope}\\n\\n\")\n",
    "    print(f\"Composites:\\n{composites}\\n\\n\")\n",
    "    print(f\"Composites:\\n{composites}\\n\\n\")\n",
    "    print(f\"Dataset:\\n{dataset}\\n\\n\")\n",
    "    \n",
    "    dataset = dataset.unify_chunks()\n",
    "    \n",
    "    svm = load_SVMS(parameters[\"model_path\"])\n",
    "    features = list(svm.preprocessing_metadata.keys())\n",
    "    dataset_flattened = expand_bands_and_reduce(dataset, features)\n",
    "    scalers = {f : get_scaler(info[\"method\"], info[\"params\"]) for f, info in svm.preprocessing_metadata.items()}\n",
    "    to_exlude = set(['ground_truth_index', 'ground_truth_label', 'split', 'x', 'y'])\n",
    "    dataset_selected = dataset_flattened[svm.features_selected]\n",
    "\n",
    "    print(\"\\n\\nInitial xarray dataset flattened:\\n\\n\",dataset_flattened)\n",
    "    print(\"\\n\\nDataset flattened selected:\\n\\n\",dataset_selected)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process patch toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_SIZE, Y_SIZE = 1000, 1000\n",
    "CHUNK_SIZE = 250  # Chunk 250x250 for X e Y\n",
    "\n",
    "ds = xr.Dataset({\n",
    "    'var1': (['x', 'y'], da.random.random((X_SIZE, Y_SIZE), chunks=(CHUNK_SIZE, CHUNK_SIZE))),\n",
    "    'var2': (['x', 'y'], da.random.random((X_SIZE, Y_SIZE), chunks=(CHUNK_SIZE, CHUNK_SIZE))),\n",
    "    'var3': (['x', 'y'], da.random.random((X_SIZE, Y_SIZE), chunks=(CHUNK_SIZE, CHUNK_SIZE)))\n",
    "})\n",
    "\n",
    "def predict_svm(var1, var2, var3):\n",
    "    \"\"\"Restituisce 3 array di output con la stessa forma dell'input\"\"\"\n",
    "    pred1 = var1 * 0.5 + var2 * 0.3 - var3 * 0.2  # SVM 1\n",
    "    pred2 = var1 * 0.4 - var2 * 0.1 + var3 * 0.7  # SVM 2\n",
    "    pred3 = var1 * 0.6 + var2 * 0.2 + var3 * 0.2  # SVM 3\n",
    "    return np.stack([pred1, pred2, pred3], axis=0)\n",
    "\n",
    "predictions = xr.apply_ufunc(\n",
    "    predict_svm,\n",
    "    ds['var1'], ds['var2'], ds['var3'],\n",
    "    input_core_dims=[(\"x\", \"y\")] * 3,\n",
    "    output_core_dims=[(\"svm\", \"x\", \"y\")],\n",
    "    vectorize=True,\n",
    "    dask=\"allowed\",\n",
    "    output_dtypes=[np.float64],\n",
    ")\n",
    "print(predictions)\n",
    "\n",
    "pred_ds = xr.Dataset({\n",
    "    'svm1': ([\"x\", \"y\"], predictions.sel(svm=0).data),\n",
    "    'svm2': ([\"x\", \"y\"], predictions.sel(svm=1).data),\n",
    "    'svm3': ([\"x\", \"y\"], predictions.sel(svm=2).data),\n",
    "})\n",
    "\n",
    "print(pred_ds)\n",
    "pred_ds = pred_ds.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process single pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pixel(features, scalers, svm, x, y, args):\n",
    "\n",
    "    preprocessed_features = {}\n",
    "    print(f\"Processing point ({x}, {y})\")\n",
    "\n",
    "    # Step 1: Preprocess Features\n",
    "    for feature_name, feature_value in features.items():\n",
    "        \n",
    "        if feature_name in [\"x\", \"y\"]:\n",
    "            continue  # Skip coordinates\n",
    "        \n",
    "        feature_value = feature_value.reshape(-1, 1)  # Ensure 2D input for scaler\n",
    "        curr_scaler = scalers[feature_name]\n",
    "        \n",
    "        if isinstance(curr_scaler, DegreeToSinCos):\n",
    "            sin_col, cos_col = curr_scaler.transform(feature_value)\n",
    "            preprocessed_features[f\"{feature_name}_sin\"] = sin_col.flatten()\n",
    "            preprocessed_features[f\"{feature_name}_cos\"] = cos_col.flatten()\n",
    "            print(f\"Scaled {feature_name} to sin: {sin_col}, cos: {cos_col}\")\n",
    "            \n",
    "        else:    \n",
    "            preprocessed_features[feature_name] = curr_scaler.transform(feature_value).flatten()\n",
    "            print(f\"Scaled {feature_name}: {preprocessed_features[feature_name]} with {curr_scaler}\")\n",
    "\n",
    "    # Convert preprocessed_features dictionary to an ordered NumPy array\n",
    "    feature_array = np.array([preprocessed_features[key] for key in preprocessed_features])\n",
    "\n",
    "    # Step 2: Apply PCA if enabled\n",
    "    if svm.pca_metadata is not None:\n",
    "        pca = args['pca']\n",
    "        feature_names = pca.feature_names_in_\n",
    "\n",
    "        # Ensure the correct order of features\n",
    "        if not all(f in preprocessed_features for f in feature_names):\n",
    "            raise ValueError(f\"Some PCA features are missing: {set(feature_names) - set(preprocessed_features)}\")\n",
    "\n",
    "        # Extract the features in the correct order\n",
    "        pca_input = np.array([preprocessed_features[f] for f in feature_names]).reshape(1, -1)  # PCA needs 2D\n",
    "\n",
    "        # Apply PCA transformation\n",
    "        pca_transformed = pca.transform(pca_input).flatten()\n",
    "        pca_transformed = pca_transformed.reshape(1, -1)\n",
    "\n",
    "        # Replace feature_array with PCA result\n",
    "        feature_array = pca_transformed\n",
    "\n",
    "    \n",
    "    output = {\"x\":x, \"y\":y}\n",
    "\n",
    "    # 1. Prediction with Multiclass SVM not calibrated\n",
    "    svm.use_binary_svms = False\n",
    "    svm.use_multiclass_svm_calibrated = False\n",
    "    svm.use_binary_svms_with_softmax = False\n",
    "    output.update({f\"predictions_SvmMc\" : svm.predict(feature_array)})\n",
    "    \n",
    "    # 2. Prediction with Multiclass SVM calibrated\n",
    "    svm.use_binary_svms = False\n",
    "    svm.use_multiclass_svm_calibrated = True\n",
    "    svm.use_binary_svms_with_softmax = False\n",
    "    output.update({f\"predictions_SvmMcCal\" : svm.predict(feature_array)})\n",
    "    \n",
    "    # 3. Prediction with Binary SVMs\n",
    "    svm.use_binary_svms = True\n",
    "    svm.use_multiclass_svm_calibrated = False\n",
    "    svm.use_binary_svms_with_softmax = False\n",
    "    output.update({f\"predictions_SvmsBin\" : svm.predict(feature_array)})\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "args = {}\n",
    "if svm.pca_metadata is not None:\n",
    "    pca_model = PCA()\n",
    "    pca_model.components_ = svm.pca_metadata[\"params\"][\"components_\"]\n",
    "    pca_model.n_components_ = svm.pca_metadata[\"params\"][\"n_components_\"]\n",
    "    pca_model.explained_variance_ = svm.pca_metadata[\"params\"][\"explained_variance_\"]\n",
    "    pca_model.singular_values_ = svm.pca_metadata[\"params\"][\"singular_values_\"]\n",
    "    pca_model.mean_ = svm.pca_metadata[\"params\"][\"mean_\"]\n",
    "    pca_model.n_samples_ = svm.pca_metadata[\"params\"][\"n_samples_\"]\n",
    "    pca_model.noise_variance_ = svm.pca_metadata[\"params\"][\"noise_variance_\"]\n",
    "    pca_model.n_features_in_ = svm.pca_metadata[\"params\"][\"n_features_in_\"]\n",
    "    pca_model.feature_names_in_ = svm.pca_metadata[\"params\"][\"feature_names_in_\"]\n",
    "\n",
    "    args[\"pca\"] = pca_model\n",
    "\n",
    "\n",
    "first_pixel = dataset_selected.isel(x=0, y=0).compute()\n",
    "print(f\"First pixel:\\n\\n {first_pixel}\")\n",
    "\n",
    "# Extract features for a single pixel\n",
    "first_pixel_values = {var: first_pixel[var].data for var in dataset_selected.data_vars}\n",
    "print(f\"\\n\\nFirst pixel values:\\n\\n {first_pixel}\")\n",
    "\n",
    "output = process_pixel(first_pixel_values, scalers, svm, args = args, x=0, y=1)\n",
    "\n",
    "print(f\"Predictions for pixel (0,1): {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a single patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svm_patch(features_array, feature_names, scalers, svm, x_coords, y_coords, args, output_images):\n",
    "\n",
    "    patch_x, patch_y = features_array.shape[1:]  # Get spatial patch size\n",
    "    preprocessed_features = np.zeros_like(features_array)  # To store processed values\n",
    "    \n",
    "    print(f\"Processing patch of shape {features_array.shape} at X range {x_coords} and Y range {y_coords}\")\n",
    "\n",
    "    # Step 1: Preprocess Features\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_values = features_array[i, :, :].reshape(-1, 1)  # Flatten patch for batch processing\n",
    "        curr_scaler = scalers[feature_name]\n",
    "\n",
    "        if isinstance(curr_scaler, DegreeToSinCos):\n",
    "            sin_col, cos_col = curr_scaler.transform(feature_values)\n",
    "            preprocessed_features[i, :, :] = sin_col.reshape(patch_x, patch_y)  # Store transformed values\n",
    "        else:\n",
    "            preprocessed_features[i, :, :] = curr_scaler.transform(feature_values).reshape(patch_x, patch_y)\n",
    "\n",
    "    # Step 2: Apply PCA if enabled\n",
    "    if svm.pca_metadata is not None:\n",
    "        pca = args['pca']\n",
    "        feature_names_pca = pca.feature_names_in_\n",
    "\n",
    "        # Ensure correct feature order for PCA\n",
    "        feature_indices = [feature_names.index(f) for f in feature_names_pca]\n",
    "        pca_input = preprocessed_features[feature_indices, :, :].reshape(len(feature_names_pca), -1).T  # (n_samples, n_features)\n",
    "\n",
    "        # Apply PCA\n",
    "        pca_transformed = pca.transform(pca_input).T.reshape(pca.n_components_, patch_x, patch_y)\n",
    "\n",
    "        # Replace feature array with PCA result\n",
    "        preprocessed_features = pca_transformed\n",
    "\n",
    "    # Step 3: Make predictions using SVMs\n",
    "    preprocessed_features_flat = preprocessed_features.reshape(preprocessed_features.shape[0], -1).T  # (n_samples, n_features)\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    # 1. Multiclass SVM (not calibrated)\n",
    "    svm.use_binary_svms = False\n",
    "    svm.use_multiclass_svm_calibrated = False\n",
    "    svm.use_binary_svms_with_softmax = False\n",
    "    predictions_SvmMc = svm.predict(preprocessed_features_flat).reshape(patch_x, patch_y)\n",
    "\n",
    "    # 2. Multiclass SVM (calibrated)\n",
    "    svm.use_multiclass_svm_calibrated = True\n",
    "    predictions_SvmMcCal = svm.predict(preprocessed_features_flat).reshape(patch_x, patch_y)\n",
    "\n",
    "    # 3. Binary SVMs\n",
    "    svm.use_binary_svms = True\n",
    "    svm.use_multiclass_svm_calibrated = False\n",
    "    predictions_SvmsBin = svm.predict(preprocessed_features_flat).reshape(patch_x, patch_y)\n",
    "\n",
    "    # Convert label descriptions to label IDs\n",
    "    def map_labels_to_ids(predictions):\n",
    "        return np.vectorize(lambda label: label2id.get(label, 0))(predictions)  # Default to 0 if label not found\n",
    "\n",
    "    predictions_SvmMc = map_labels_to_ids(predictions_SvmMc)\n",
    "    predictions_SvmMcCal = map_labels_to_ids(predictions_SvmMcCal)\n",
    "    predictions_SvmsBin = map_labels_to_ids(predictions_SvmsBin)\n",
    "\n",
    "    # Write to output images\n",
    "    output_images[\"predictions_SvmMc\"][y_indexes[0]:y_indexes[-1]+1, x_indexes[0]:x_indexes[-1]+1] = predictions_SvmMc\n",
    "    output_images[\"predictions_SvmMcCal\"][y_indexes[0]:y_indexes[-1]+1, x_indexes[0]:x_indexes[-1]+1] = predictions_SvmMcCal\n",
    "    output_images[\"predictions_SvmsBin\"][y_indexes[0]:y_indexes[-1]+1, x_indexes[0]:x_indexes[-1]+1] = predictions_SvmsBin\n",
    "\n",
    "    print(f\"Written predictions to output images for patch {x_indexes[0]}:{x_indexes[-1]+1}, {y_indexes[0]}:{y_indexes[-1]+1}\")\n",
    "\n",
    "\n",
    "\n",
    "args = {}\n",
    "if svm.pca_metadata is not None:\n",
    "    pca_model = PCA()\n",
    "    pca_model.components_ = svm.pca_metadata[\"params\"][\"components_\"]\n",
    "    pca_model.n_components_ = svm.pca_metadata[\"params\"][\"n_components_\"]\n",
    "    pca_model.explained_variance_ = svm.pca_metadata[\"params\"][\"explained_variance_\"]\n",
    "    pca_model.singular_values_ = svm.pca_metadata[\"params\"][\"singular_values_\"]\n",
    "    pca_model.mean_ = svm.pca_metadata[\"params\"][\"mean_\"]\n",
    "    pca_model.n_samples_ = svm.pca_metadata[\"params\"][\"n_samples_\"]\n",
    "    pca_model.noise_variance_ = svm.pca_metadata[\"params\"][\"noise_variance_\"]\n",
    "    pca_model.n_features_in_ = svm.pca_metadata[\"params\"][\"n_features_in_\"]\n",
    "    pca_model.feature_names_in_ = svm.pca_metadata[\"params\"][\"feature_names_in_\"]\n",
    "\n",
    "    args[\"pca\"] = pca_model\n",
    "\n",
    "id2label = {id : row['description'] for id, row in pd.read_csv(parameters[\"labels_path\"]).iterrows()}\n",
    "label2id = {row['description'] : id for id, row in pd.read_csv(parameters[\"labels_path\"]).iterrows()}\n",
    "args['label2id'] = label2id\n",
    "\n",
    "def create_image_for_classification(dataset):\n",
    "    output_shape = (10980, 10980)\n",
    "    return xr.DataArray(\n",
    "        np.full(output_shape, np.nan, dtype=np.uint8),\n",
    "        dims=(\"y\", \"x\"),\n",
    "        coords={\"y\": dataset[\"y\"], \"x\": dataset[\"x\"]},\n",
    "    ).assign_coords({\"spatial_ref\": dataset.spatial_ref})\n",
    "\n",
    "\n",
    "output_images = {\n",
    "    \"predictions_SvmMc\": create_output_image(),\n",
    "    \"predictions_SvmMcCal\": create_output_image(),\n",
    "    \"predictions_SvmsBin\": create_output_image(),\n",
    "    \"predictions_SvmsBinSm\": create_output_image(),\n",
    "}\n",
    "\n",
    "chunk_size = 50\n",
    "\n",
    "\n",
    "# Extract patch from dataset\n",
    "patch = dataset_selected.isel(\n",
    "    x=slice(0, chunk_size),\n",
    "    y=slice(0, chunk_size)\n",
    ").compute()\n",
    "\n",
    "# Convert patch to NumPy array of shape (n_features, patch_x, patch_y)\n",
    "feature_names = list(patch.data_vars.keys())\n",
    "features_array = np.array([patch[var].data for var in feature_names])\n",
    "print(f\"Features array shape: {features_array.shape}\")\n",
    "\n",
    "# Extract x and y indexes\n",
    "x_indexes = np.arange(0, chunk_size)\n",
    "y_indexes = np.arange(0, chunk_size)\n",
    "\n",
    "# Test function on the patch\n",
    "output_patch = predict_svm_patch(features_array, feature_names, scalers, svm, x_indexes, y_indexes, args, output_images)\n",
    "print(\"Patch Predictions:\")\n",
    "print(output_patch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process chunks in parallel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(\n",
    "    features_array: np.ndarray, \n",
    "    feature_names: list, \n",
    "    scalers: dict, \n",
    "    svm_path : str, \n",
    "    args: dict, \n",
    "\n",
    ") -> tuple:\n",
    "   \n",
    "    svm = load_SVMS(svm_path)\n",
    "    patch_x, patch_y = features_array.shape[1:]  # Patch size\n",
    "    preprocessed_features = np.zeros_like(features_array)\n",
    "\n",
    "    # Step 1: Preprocess Features\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        \n",
    "        feature_values = features_array[i, :, :].reshape(-1, 1)  # Flatten for batch processing\n",
    "        curr_scaler = scalers[feature_name]\n",
    "        preprocessed_features[i, :, :] = curr_scaler.transform(feature_values).reshape(patch_x, patch_y)\n",
    "\n",
    "    # Step 2: Apply PCA if enabled\n",
    "    if svm.pca is not None:\n",
    "        pca = args['pca']\n",
    "        feature_names_pca = pca.feature_names_in_\n",
    "        feature_indices = [feature_names.index(f) for f in feature_names_pca]\n",
    "        pca_input = preprocessed_features[feature_indices, :, :].reshape(len(feature_names_pca), -1).T \n",
    "    \n",
    "        # Apply PCA\n",
    "        pca_transformed = pca.transform(pca_input).T.reshape(pca.n_components_, patch_x, patch_y)\n",
    "        preprocessed_features = pca_transformed\n",
    "\n",
    "    # Step 3: Make Predictions\n",
    "    preprocessed_features_flat = preprocessed_features.reshape(preprocessed_features.shape[0], -1).T  \n",
    "\n",
    "    label_mapper = args['label_mapper']\n",
    "    # --- Multiclass SVM (not calibrated) ---\n",
    "    svm.use_binary_svms_with_softmax = False\n",
    "    svm.use_binary_svms = False\n",
    "    svm.use_multiclass_svm_calibrated = False\n",
    "    predictions_SvmMc = label_mapper(svm.predict(preprocessed_features_flat).reshape(patch_x, patch_y))\n",
    "    probabilities_SvmMc = svm.predict_proba(preprocessed_features_flat).reshape(patch_x, patch_y, -1)\n",
    "\n",
    "    # --- Multiclass SVM (calibrated) ---\n",
    "    svm.use_binary_svms_with_softmax = False\n",
    "    svm.use_binary_svms = False\n",
    "    svm.use_multiclass_svm_calibrated = True\n",
    "    predictions_SvmMcCal = label_mapper(svm.predict(preprocessed_features_flat).reshape(patch_x, patch_y))\n",
    "    probabilities_SvmMcCal = svm.predict_proba(preprocessed_features_flat).reshape(patch_x, patch_y, -1)\n",
    "\n",
    "    # --- Binary SVMs ---\n",
    "    svm.use_binary_svms_with_softmax = False\n",
    "    svm.use_binary_svms = True\n",
    "    svm.use_multiclass_svm_calibrated = False\n",
    "    predictions_SvmsBin = label_mapper(svm.predict(preprocessed_features_flat).reshape(patch_x, patch_y))\n",
    "    probabilities_SvmsBin = svm.predict_proba(preprocessed_features_flat).reshape(patch_x, patch_y, -1)\n",
    "\n",
    "\n",
    "    # --- Binary SVMs with Softmax---\n",
    "    svm.use_binary_svms_with_softmax = True\n",
    "    svm.use_binary_svms = True\n",
    "    svm.use_multiclass_svm_calibrated = False\n",
    "    predictions_SvmsBinSm = label_mapper(svm.predict(preprocessed_features_flat).reshape(patch_x, patch_y))\n",
    "    probabilities_SvmsBinSm = svm.predict_proba(preprocessed_features_flat).reshape(patch_x, patch_y, -1)\n",
    "\n",
    "    #print(f\"Processed patch {x_indexes[0]}:{x_indexes[-1]+1}, {y_indexes[0]}:{y_indexes[-1]+1}\")\n",
    "\n",
    "    del feature_values, preprocessed_features, preprocessed_features_flat, features_array\n",
    "\n",
    "    return predictions_SvmMc, predictions_SvmMcCal, predictions_SvmsBin, predictions_SvmsBinSm, probabilities_SvmMc, probabilities_SvmMcCal, probabilities_SvmsBin, probabilities_SvmsBinSm\n",
    "\n",
    "def create_output_image(dims_labels):\n",
    "    output_shape = (len(dims_labels), 10980, 10980)  # Define the full image size\n",
    "    return xr.DataArray(\n",
    "        np.full(output_shape, np.nan, dtype=np.uint8),  # Use uint8 for classification\n",
    "        dims=(\"band\", \"y\", \"x\"),\n",
    "        coords={\"band\": dims_labels, \"y\": dataset[\"y\"], \"x\": dataset[\"x\"]},\n",
    "    ).assign_coords({\"spatial_ref\": dataset.spatial_ref})\n",
    "    \n",
    "band_dim_labels = svm.classes.tolist() + [\"labels\"]\n",
    "print(f\"band dim labels: {band_dim_labels}\")\n",
    "output_images = {\n",
    "    \"predictions_SvmMc\": create_output_image(band_dim_labels),\n",
    "    \"predictions_SvmMcCal\": create_output_image(band_dim_labels),\n",
    "    \"predictions_SvmsBin\": create_output_image(band_dim_labels),\n",
    "    \"predictions_SvmsBinSm\": create_output_image(band_dim_labels),\n",
    "}\n",
    "    \n",
    "args = {}\n",
    "\n",
    "if svm.pca_metadata is not None:\n",
    "    pca_model = PCA()\n",
    "    pca_model.components_ = svm.pca_metadata[\"params\"][\"components_\"]\n",
    "    pca_model.n_components_ = svm.pca_metadata[\"params\"][\"n_components_\"]\n",
    "    pca_model.explained_variance_ = svm.pca_metadata[\"params\"][\"explained_variance_\"]\n",
    "    pca_model.singular_values_ = svm.pca_metadata[\"params\"][\"singular_values_\"]\n",
    "    pca_model.mean_ = svm.pca_metadata[\"params\"][\"mean_\"]\n",
    "    pca_model.n_samples_ = svm.pca_metadata[\"params\"][\"n_samples_\"]\n",
    "    pca_model.noise_variance_ = svm.pca_metadata[\"params\"][\"noise_variance_\"]\n",
    "    pca_model.n_features_in_ = svm.pca_metadata[\"params\"][\"n_features_in_\"]\n",
    "    pca_model.feature_names_in_ = svm.pca_metadata[\"params\"][\"feature_names_in_\"]\n",
    "\n",
    "    args[\"pca\"] = pca_model\n",
    "\n",
    "feature_names = list(dataset_selected.data_vars)\n",
    "n_features = len(feature_names)\n",
    "patch_x, patch_y = 32, 32\n",
    "labels_df =  pd.read_csv(parameters[\"labels_path\"])\n",
    "id2label = {row['LC_code'] : row['description'] for id, row in labels_df.iterrows()}\n",
    "label2id = {label : id for id, label in id2label.items()}\n",
    "label_mapper = np.vectorize(lambda label: label2id[label])\n",
    "\n",
    "args[\"label_mapper\"] = label_mapper\n",
    "\n",
    "# Create a random features_array of shape (n_features, patch_x, patch_y)\n",
    "features_array = np.random.rand(n_features, patch_x, patch_y).astype(np.float32)\n",
    "print(f\"Features names:\", feature_names)\n",
    "print(f\"Input dataset: {features_array.shape}\", features_array)\n",
    "print(f\"ID2label: \", id2label)\n",
    "a = process_chunk(\n",
    "                features_array=features_array,\n",
    "                feature_names=feature_names,\n",
    "                scalers=scalers,\n",
    "                svm_path=parameters['model_path'],\n",
    "                args = args\n",
    "                )\n",
    "\n",
    "for i in range(len(a)):\n",
    "    print(a[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_write_chunk(\n",
    "    dataset, x_start, y_start, chunk_size, feature_names, scalers, svm_path, args, output_paths\n",
    "):\n",
    "    patch = dataset.isel(\n",
    "        x=slice(x_start, x_start + chunk_size),\n",
    "        y=slice(y_start, y_start + chunk_size)\n",
    "    )\n",
    "\n",
    "    features_array = np.stack([\n",
    "        patch[var].values for var in feature_names\n",
    "    ])  # Shape: (n_features, patch_y, patch_x)\n",
    "\n",
    "    results = process_chunk(\n",
    "        features_array=features_array,\n",
    "        feature_names=feature_names,\n",
    "        scalers=scalers,\n",
    "        svm_path=svm_path,\n",
    "        args=args\n",
    "    )\n",
    "\n",
    "    predictions = {\n",
    "        \"predictions_SvmMc\": results[0],\n",
    "        \"predictions_SvmMcCal\": results[1],\n",
    "        \"predictions_SvmsBin\": results[2],\n",
    "        \"predictions_SvmsBinSm\": results[3]\n",
    "    }\n",
    "\n",
    "    probs = {\n",
    "        \"predictions_SvmMc\": results[4],\n",
    "        \"predictions_SvmMcCal\": results[5],\n",
    "        \"predictions_SvmsBin\": results[6],\n",
    "        \"predictions_SvmsBinSm\": results[7]\n",
    "    }\n",
    "\n",
    "    for key in predictions:\n",
    "        pred = predictions[key]\n",
    "        prob = probs[key]\n",
    "        pred = pred.astype(np.uint8)\n",
    "        prob = (prob * 255).astype(np.uint8)\n",
    "\n",
    "        stacked = np.concatenate(\n",
    "            [prob, pred[:, :, np.newaxis]], axis=-1  # shape: (patch_y, patch_x, n_classes+1)\n",
    "        )\n",
    "        write_chunk_to_tif(output_paths[key], stacked, args['band_labels'], x_start, y_start)\n",
    "\n",
    "\n",
    "\n",
    "def create_empty_output(name, template, dim_labels, dtype=np.uint8):\n",
    "    shape = (len(dim_labels), template.sizes[\"y\"], template.sizes[\"x\"])\n",
    "    data = np.full(shape, np.nan, dtype=dtype)\n",
    "    da = xr.DataArray(\n",
    "        data,\n",
    "        dims=(\"band\", \"y\", \"x\"),\n",
    "        coords={\"band\": dim_labels, \"y\": template[\"y\"], \"x\": template[\"x\"]},\n",
    "        attrs={\"transform\": template.rio.transform()},\n",
    "    ).rio.write_crs(template.rio.crs)\n",
    "    da.rio.to_raster(f\"{name}.tif\")\n",
    "    return f\"{name}.tif\"\n",
    "\n",
    "def write_chunk_to_tif(tif_path, chunk_array, bands, x_start, y_start):\n",
    "    with rasterio.open(tif_path, \"r+\", lock=False) as dst:\n",
    "        for i, band in enumerate(bands):\n",
    "            window = Window(x_start, y_start, chunk_array.shape[1], chunk_array.shape[0])\n",
    "            dst.write(chunk_array[:, :, i].astype(dst.dtypes[0]), indexes=i+1, window=window)\n",
    "\n",
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(os.cpu_count() or 2),\n",
    ") as client:\n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "\n",
    "    args = {}\n",
    "\n",
    "    if svm.pca_metadata is not None:\n",
    "        pca_model = PCA()\n",
    "        pca_model.components_ = svm.pca_metadata[\"params\"][\"components_\"]\n",
    "        pca_model.n_components_ = svm.pca_metadata[\"params\"][\"n_components_\"]\n",
    "        pca_model.explained_variance_ = svm.pca_metadata[\"params\"][\"explained_variance_\"]\n",
    "        pca_model.singular_values_ = svm.pca_metadata[\"params\"][\"singular_values_\"]\n",
    "        pca_model.mean_ = svm.pca_metadata[\"params\"][\"mean_\"]\n",
    "        pca_model.n_samples_ = svm.pca_metadata[\"params\"][\"n_samples_\"]\n",
    "        pca_model.noise_variance_ = svm.pca_metadata[\"params\"][\"noise_variance_\"]\n",
    "        pca_model.n_features_in_ = svm.pca_metadata[\"params\"][\"n_features_in_\"]\n",
    "        pca_model.feature_names_in_ = svm.pca_metadata[\"params\"][\"feature_names_in_\"]\n",
    "\n",
    "        args[\"pca\"] = pca_model\n",
    "\n",
    "    feature_names = list(dataset_selected.data_vars)\n",
    "    n_features = len(feature_names)\n",
    "    patch_x, patch_y = 32, 32\n",
    "    labels_df =  pd.read_csv(parameters[\"labels_path\"])\n",
    "    id2label = {row['LC_code'] : row['description'] for id, row in labels_df.iterrows()}\n",
    "    label2id = {label : id for id, label in id2label.items()}\n",
    "    label_mapper = np.vectorize(lambda label: label2id[label])\n",
    "\n",
    "    args[\"label_mapper\"] = label_mapper\n",
    "\n",
    "    chunk_size = 512\n",
    "    x_size = dataset.sizes[\"x\"]\n",
    "    y_size = dataset.sizes[\"y\"]\n",
    "    band_dim_labels = svm.classes.tolist() + [\"labels\"]\n",
    "\n",
    "    # Crea immagini output vuote\n",
    "    output_paths = {\n",
    "        name: create_empty_output(name, dataset, band_dim_labels) for name in [\n",
    "            \"predictions_SvmMc\", \"predictions_SvmMcCal\", \"predictions_SvmsBin\", \"predictions_SvmsBinSm\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    tasks = []\n",
    "    args['band_labels'] = band_dim_labels\n",
    "    for y_start in range(0, y_size, chunk_size):\n",
    "        for x_start in range(0, x_size, chunk_size):\n",
    "            print(f\"Processing chunk from {y_start} to {x_start}\")\n",
    "            task = process_and_write_chunk(\n",
    "                dataset=dataset_selected,\n",
    "                x_start=x_start,\n",
    "                y_start=y_start,\n",
    "                chunk_size=chunk_size,\n",
    "                feature_names=feature_names,\n",
    "                scalers=scalers,\n",
    "                svm_path=parameters[\"model_path\"],\n",
    "                args=args,\n",
    "                output_paths=output_paths\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a)):\n",
    "    print(a[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(os.cpu_count() or 2),\n",
    ") as client:\n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "\n",
    "    print(\"\\n\\nInitial xarray dataset flattened:\\n\\n\",dataset_flattened)\n",
    "    \n",
    "    args = {}\n",
    "    \n",
    "    if svm.pca_metadata is not None:\n",
    "        pca_model = PCA()\n",
    "        pca_model.components_ = svm.pca_metadata[\"params\"][\"components_\"]\n",
    "        pca_model.n_components_ = svm.pca_metadata[\"params\"][\"n_components_\"]\n",
    "        pca_model.explained_variance_ = svm.pca_metadata[\"params\"][\"explained_variance_\"]\n",
    "        pca_model.singular_values_ = svm.pca_metadata[\"params\"][\"singular_values_\"]\n",
    "        pca_model.mean_ = svm.pca_metadata[\"params\"][\"mean_\"]\n",
    "        pca_model.n_samples_ = svm.pca_metadata[\"params\"][\"n_samples_\"]\n",
    "        pca_model.noise_variance_ = svm.pca_metadata[\"params\"][\"noise_variance_\"]\n",
    "        pca_model.n_features_in_ = svm.pca_metadata[\"params\"][\"n_features_in_\"]\n",
    "        pca_model.feature_names_in_ = svm.pca_metadata[\"params\"][\"feature_names_in_\"]\n",
    "\n",
    "        args[\"pca\"] = pca_model\n",
    "\n",
    "    # Load class mappings\n",
    "\n",
    "    # Define the patch size\n",
    "    chunk_size = parameters[\"chunk_size\"]\n",
    "    dataset_selected = dataset_selected.chunk({\"x\": chunk_size, \"y\": chunk_size})\n",
    "    feature_names = list(dataset_selected.data_vars.keys())\n",
    "\n",
    "    tasks = []\n",
    "    i = 0\n",
    "    limited = parameters[\"chunks_limit\"] is not None\n",
    "    limit = parameters[\"chunks_limit\"]\n",
    "    chunk_positions = []  # Store the chunk positions in the same order as tasks\n",
    "\n",
    "    print(\"Processing dataset chuncks:\")\n",
    "    print(f\" - Limit: {limit}\")\n",
    "    \n",
    "    predictions_da = xr.apply_ufunc(\n",
    "        dataset_selected,\n",
    "        kwargs={\n",
    "            \"svm\": load_SVMS(parameters[\"model_path\"]),\n",
    "            \"scalers\": scalers,\n",
    "            \"apply_pca\": True,\n",
    "            \"pca\": pca_model,\n",
    "        },\n",
    "        input_core_dims=[],  # one [] per input feature\n",
    "        output_core_dims=[[\"band\"]],  # output has an extra \"band\" dimension\n",
    "        vectorize=True,               # essential since we have multiple arrays\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[np.float32],\n",
    "        output_sizes={\"band\": 1 + len(svm.classes)},  # 1 prediction band + N probability bands\n",
    "    )\n",
    "\n",
    "            \n",
    "    print(f\"Predictions - Computing {len(tasks)} Dask's task\")\n",
    "    results = dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask Client\n",
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(os.cpu_count() or 2),\n",
    ") as client:\n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "    band_dim_labels = svm.classes.tolist() + [\"labels\"]\n",
    "\n",
    "    def create_output_image():\n",
    "        output_shape = (len(band_dim_labels), 10980, 10980)  # Define the full image size\n",
    "        return xr.DataArray(\n",
    "            np.full(output_shape, np.nan, dtype=np.uint8),  # Use uint8 for classification\n",
    "            dims=(\"band\", \"y\", \"x\"),\n",
    "            coords={\"band\": band_dim_labels, \"y\": dataset[\"y\"], \"x\": dataset[\"x\"]},\n",
    "        ).assign_coords({\"spatial_ref\": dataset.spatial_ref})\n",
    "        \n",
    "    output_images = {\n",
    "        \"predictions_SvmMc\": create_output_image(),\n",
    "        \"predictions_SvmMcCal\": create_output_image(),\n",
    "        \"predictions_SvmsBin\": create_output_image(),\n",
    "        \"predictions_SvmsBinSm\": create_output_image(),\n",
    "    }\n",
    "\n",
    "    print(\"Saving land cover maps to disk\")\n",
    "\n",
    "    # Ensure the number of results matches the chunk positions\n",
    "    if len(results) != len(chunk_positions):\n",
    "        print(\"Mismatch between computed results and expected chunk positions!\")\n",
    "        raise ValueError(\"Computed results do not match expected chunk positions.\")\n",
    "\n",
    "    # Assign values based on chunk positions\n",
    "    for i, ((predictions_SvmMc, \n",
    "            predictions_SvmMcCal, \n",
    "            predictions_SvmsBin, \n",
    "            #predictions_SvmsBinSm,\n",
    "            predProb_SvmMc, \n",
    "            predProb_SvmMcCal,\n",
    "            predProb_SvmsBin,\n",
    "            #predProb_SvmsBinSm\n",
    "            ), \n",
    "            (x_start, y_start)) in enumerate(zip(results, chunk_positions)):\n",
    "        \n",
    "        print(f\"Writing results for chunk {i+1} at x={x_start}-{x_start+chunk_size}, y={y_start}-{y_start+chunk_size}\")\n",
    "\n",
    "        # Get Numpy arrays from Dask arrays\n",
    "        pred_svm_mc = predictions_SvmMc.compute()\n",
    "        pred_svm_mc_cal = predictions_SvmMcCal.compute()\n",
    "        pred_svms_bin = predictions_SvmsBin.compute()\n",
    "        #pred_svms_bin_sm = predictions_SvmsBinSm.compute()\n",
    "        predProb_svm_mc = predProb_SvmMc.compute()\n",
    "        predProb_svm_mc_cal = predProb_SvmMcCal.compute()\n",
    "        predProb_svms_bin = predProb_SvmsBin.compute()\n",
    "        #predProb_svms_bin_sm = predProb_SvmsBinSm.compute()\n",
    "\n",
    "        output_images[\"predictions_SvmMc\"].sel(band = 'labels').isel(\n",
    "            y=slice(y_start, y_start + chunk_size), x=slice(x_start, x_start + chunk_size)\n",
    "        ).data[:] = pred_svm_mc\n",
    "\n",
    "        output_images[\"predictions_SvmMcCal\"].sel(band = 'labels').isel(\n",
    "            y=slice(y_start, y_start + chunk_size), x=slice(x_start, x_start + chunk_size)\n",
    "        ).data[:] = pred_svm_mc_cal\n",
    "\n",
    "        output_images[\"predictions_SvmsBin\"].sel(band = 'labels').isel(\n",
    "            y=slice(y_start, y_start + chunk_size), x=slice(x_start, x_start + chunk_size)\n",
    "        ).data[:] = pred_svms_bin\n",
    "\n",
    "        #output_images[\"predictions_SvmsBinSm\"].sel(band = 'labels').isel(\n",
    "            #y=slice(y_start, y_start + chunk_size), x=slice(x_start, x_start + chunk_size)\n",
    "        #).data[:] = pred_svms_bin_sm\n",
    "\n",
    "        # Write class-wise probabilities\n",
    "        for class_index, class_name in enumerate(svm.classes):\n",
    "            output_images[\"predictions_SvmMc\"].sel(band=class_name).isel(\n",
    "                y=slice(y_start, y_start + chunk_size),\n",
    "                x=slice(x_start, x_start + chunk_size)\n",
    "            ).data[:] = (predProb_svm_mc[:, :, class_index] * 255).astype(np.uint8)\n",
    "\n",
    "            output_images[\"predictions_SvmMcCal\"].sel(band=class_name).isel(\n",
    "                y=slice(y_start, y_start + chunk_size),\n",
    "                x=slice(x_start, x_start + chunk_size)\n",
    "            ).data[:] = (predProb_svm_mc_cal[:, :, class_index] * 255).astype(np.uint8)\n",
    "\n",
    "            output_images[\"predictions_SvmsBin\"].sel(band=class_name).isel(\n",
    "                y=slice(y_start, y_start + chunk_size),\n",
    "                x=slice(x_start, x_start + chunk_size)\n",
    "            ).data[:] = (predProb_svms_bin[:, :, class_index] * 255).astype(np.uint8)\n",
    "            \n",
    "            #output_images[\"predictions_SvmsBinSm\"].sel(band=class_name).isel(\n",
    "            #    y=slice(y_start, y_start + chunk_size),\n",
    "            #    x=slice(x_start, x_start + chunk_size)\n",
    "            #).data[:] = (predProb_svms_bin_sm[:, :, class_index] * 255).astype(np.uint8)\n",
    "            \n",
    "    print(\"Writing complete. Now saving images.\")\n",
    "\n",
    "    storage_paths = []\n",
    "    \n",
    "    for name, image in output_images.items():\n",
    "        \n",
    "        output_path = f\"{parameters['classified_image_path']}_{name}.tif\"\n",
    "        image.rio.to_raster(output_path)  \n",
    "        storage_paths.append(output_path)\n",
    "          \n",
    "        print(f\"Saved classified image: {output_path}\")\n",
    "\n",
    "    print(\"All classification images saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.distributed.Client(\n",
    "    processes=False,\n",
    "    threads_per_worker=(4),\n",
    ") as client:\n",
    "    \n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "    print(\"\\n\\nInitial xarray dataset flattened:\\n\\n\",dataset_flattened)\n",
    "    \n",
    "    args = {}\n",
    "    \n",
    "    if svm.pca is not None:\n",
    "        pca_model = PCA()\n",
    "        pca_model.components_ = svm.pca[\"params\"][\"components_\"]\n",
    "        pca_model.n_components_ = svm.pca[\"params\"][\"n_components_\"]\n",
    "        pca_model.explained_variance_ = svm.pca[\"params\"][\"explained_variance_\"]\n",
    "        pca_model.singular_values_ = svm.pca[\"params\"][\"singular_values_\"]\n",
    "        pca_model.mean_ = svm.pca[\"params\"][\"mean_\"]\n",
    "        pca_model.n_samples_ = svm.pca[\"params\"][\"n_samples_\"]\n",
    "        pca_model.noise_variance_ = svm.pca[\"params\"][\"noise_variance_\"]\n",
    "        pca_model.n_features_in_ = svm.pca[\"params\"][\"n_features_in_\"]\n",
    "        pca_model.feature_names_in_ = svm.pca[\"params\"][\"feature_names_in_\"]\n",
    "        args[\"pca\"] = pca_model\n",
    "\n",
    "    transform = dataset_selected.rio.transform()\n",
    "    crs = dataset_selected.rio.crs\n",
    "    height, width = 10980, 10980\n",
    "    \n",
    "    prefix = f\"{parameters['tile_id']}_{parameters['composites_year']}_{parameters['chunks_limit']}r\"\n",
    "    svms_to_use = parameters[\"svms_to_use\"]\n",
    "    \n",
    "    prefix = f\"{parameters['tile_id']}_{parameters['composites_year']}_{parameters['chunks_limit']}r\"\n",
    "    output_paths = {}\n",
    "    if 'svmMc' in svms_to_use:\n",
    "        output_paths[\"svmMc\"] = f\"{parameters['output_path']}/{prefix}_predictions_SvmMc.tif\"\n",
    "    if 'svmMcCal' in svms_to_use:\n",
    "        output_paths[\"svmMcCal\"] = f\"{parameters['output_path']}/{prefix}_predictions_SvmMcCal.tif\"\n",
    "    if 'svmsBin' in svms_to_use:\n",
    "        output_paths[\"svmsBin\"] = f\"{parameters['output_path']}/{prefix}_predictions_SvmsBin.tif\"\n",
    "    if 'svmsBinSoftmax' in svms_to_use:\n",
    "        output_paths[\"svmsBinSoftmax\"] = f\"{parameters['output_path']}/{prefix}_predictions_SvmsBinSoftmax.tif\"\n",
    "    \n",
    "    band_labels = svm.classes.tolist() + [\"labels\"]\n",
    "    print(f\"Band labels ({len(band_labels)}): {band_labels}\")\n",
    "    \n",
    "    for name, path in output_paths.items():\n",
    "        create_empty_tif(path, width, height, len(band_labels), \"uint8\", transform, crs, band_labels)\n",
    "\n",
    "    # Define the patch size\n",
    "    chunk_size = parameters[\"chunk_size\"]\n",
    "    dataset_selected = dataset_selected.chunk({\"x\": chunk_size, \"y\": chunk_size})\n",
    "    feature_names = list(dataset_selected.data_vars.keys())\n",
    "    labels_df =  pd.read_csv(parameters[\"labels_path\"])\n",
    "    id2label = {row['LC_code'] : row['description'] for id, row in labels_df.iterrows()}\n",
    "    label2id = {label : id for id, label in id2label.items()}\n",
    "    label_mapper = np.vectorize(lambda label: label2id[label])\n",
    "\n",
    "    args[\"label_mapper\"] = label_mapper\n",
    "    tasks = []\n",
    "    i = 0\n",
    "    limited = parameters[\"chunks_limit\"] is not None\n",
    "    limit = parameters[\"chunks_limit\"]\n",
    "    svm = load_SVMS(parameters['model_path'])\n",
    "    args['svm'] = svm\n",
    "    print(\"Processing dataset chunks:\")\n",
    "    print(f\" - Limit: {limit}\")\n",
    "\n",
    "    for x_start in range(0, width, chunk_size):\n",
    "        for y_start in range(0, height, chunk_size):\n",
    "            start = time.perf_counter()\n",
    "            print(f\"Processing chunk {i}\")\n",
    "            patch = dataset_selected.isel(\n",
    "                x=slice(x_start, x_start + chunk_size),\n",
    "                y=slice(y_start, y_start + chunk_size)\n",
    "            )\n",
    "            \n",
    "            features_array = da.stack([patch[var] for var in svm.features_selected]).compute()\n",
    "            print(features_array.shape)\n",
    "            args['task_id'] = i\n",
    "            process_chunk(\n",
    "                features_array,feature_names, scalers, args ,x_start, y_start, output_paths, svms_to_use\n",
    "            )\n",
    "            \n",
    "            elapsed = time.perf_counter() - start\n",
    "            print(f\"Chunk {i} processed in {elapsed:.2f} seconds\")\n",
    "            \n",
    "            del features_array\n",
    "            i += 1\n",
    "            \n",
    "            if limited and limit == i:\n",
    "                break\n",
    "            \n",
    "        if limited and limit == i:\n",
    "            break\n",
    "\n",
    "    print(f\"Finished processing all {i} chunk(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot land coverage images and store them as .png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_mc_path = \"/media/datapart/lucazanolo/SVM/lc_maps/21KUQ_2019_1r_predictions_SvmMc.tif\"\n",
    "svm_mccal_path = \"/media/datapart/lucazanolo/SVM/lc_maps/21KUQ_2019_1r_predictions_SvmMcCal.tif\"\n",
    "svms_binary_path = \"/media/datapart/lucazanolo/SVM/lc_maps/21KUQ_2019_1r_predictions_SvmsBin.tif\"\n",
    "old_map_path = \"/media/datapart/lucazanolo/data/training_points/ESACCI-HRLC-L4-MAP-CL01-A02T21KUQ-10m-P1Y-2019-fv01.1.tif\"\n",
    "\n",
    "mc_map = load_map_with_probs_and_plot(svm_mc_path, band_labels, labels_df, \"Multiclass SVM prediction\", png_images_path)    \n",
    "#mccal_map = load_map_with_probs_and_plot(svm_mccal_path, band_labels, labels_df, \"Multiclass Calibrated SVM prediction\", png_images_path)    \n",
    "#svmsbin_map = load_map_with_probs_and_plot(svms_binary_path, band_labels, labels_df, \"Binary SVMs predictions\", png_images_path)    \n",
    "old_map = load_map_and_plot(old_map_path, labels_df, \"Previous Pipeline predictions\", png_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df =  pd.read_csv(parameters[\"labels_path\"])\n",
    "\n",
    "band_labels = svm.classes.tolist() + [\"labels\"]\n",
    "\n",
    "prefix = f\"{parameters['tile_id']}_{parameters['composites_year']}_{parameters['chunks_limit']}r\"\n",
    "\n",
    "output_paths = {\n",
    "    \"predictions_SvmMc\": f\"{parameters['output_path']}/{prefix}_predictions_SvmMc.tif\",\n",
    "    \"predictions_SvmMcCal\": f\"{parameters['output_path']}/{prefix}_predictions_SvmMcCal.tif\",\n",
    "    \"predictions_SvmsBin\": f\"{parameters['output_path']}/{prefix}_predictions_SvmsBin.tif\"\n",
    "}\n",
    "\n",
    "for name, path in output_paths.items():\n",
    "    _ = load_map_with_probs_and_plot(path, band_labels, labels_df, name, png_images_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parameters['baseline_lc_map_path'] is not None:\n",
    "    _ = load_map_and_plot(parameters['baseline_lc_map_path'], labels_df, \"Previous Pipeline predictions\", png_images_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect land cover images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_df =  pd.read_csv(parameters[\"labels_path\"])\n",
    "\n",
    "lc_map_old = load_map_with_probs_and_plot(\n",
    "    lc_map_path=\"/media/datapart/lucazanolo/data/training_points/ESACCI-HRLC-L4-MAP-CL01-A02T21KUQ-10m-P1Y-2019-fv01.1.tif\",\n",
    "    labels=[\"labels\"],\n",
    "    labels_df=labels_df,\n",
    "    title=\"Existing pipeline - LC Map\"\n",
    ")\n",
    "\n",
    "lc_map_new_svmMc = load_map_and_plot(\"/media/datapart/lucazanolo/SVM/lc_maps/best_svm/lc_maps/21KUQ_2019_Noner_predictions_SvmMc.tif\", labels_df)\n",
    "lc_map_new_svmMcCal = load_map_and_plot(\"/media/datapart/lucazanolo/SVM/lc_maps/best_svm/lc_maps/21KUQ_2019_Noner_predictions_SvmMcCal.tif\", labels_df)\n",
    "\n",
    "lc_map_new_svms = load_map_and_plot(\"/media/datapart/lucazanolo/SVM/lc_maps/best_svms/lc_maps/21KUQ_2019_Noner_predictions_SvmsBin.tif\", labels_df)\n",
    "lc_map_new_svmssoft = load_map_and_plot(\"/media/datapart/lucazanolo/SVM/lc_maps/best_svms/lc_maps/21KUQ_2019_Noner_predictions_SvmsBinSoftmax.tif\", labels_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lc_map_with_histogram(lc_array: np.ndarray, labels_df: pd.DataFrame, title: str = \"\", save_path: str = None):\n",
    "\n",
    "    # Build color map\n",
    "    color_map = {\n",
    "        row.LC_code: np.array([row.R, row.G, row.B, row.A]) / 255.0\n",
    "        for _, row in labels_df.iterrows()\n",
    "    }\n",
    "\n",
    "    # Generate RGBA image\n",
    "    rgba_image = map_lc_codes_to_rgba(lc_array, color_map)\n",
    "\n",
    "    # Compute class histogram\n",
    "    unique_codes, counts = np.unique(lc_array, return_counts=True)\n",
    "\n",
    "    descriptions = []\n",
    "    bar_colors = []\n",
    "\n",
    "    for code in unique_codes:\n",
    "        row = labels_df[labels_df['LC_code'] == code]\n",
    "        if not row.empty:\n",
    "            desc = row['description'].values[0]\n",
    "            descriptions.append(desc)\n",
    "            bar_colors.append(row[['R', 'G', 'B']].values[0] / 255.0)\n",
    "        else:\n",
    "            descriptions.append(f\"Code {code}\")\n",
    "            bar_colors.append([0.5, 0.5, 0.5])  # Unknown code\n",
    "\n",
    "    # Create combined figure with constrained_layout to align widths\n",
    "    fig, axes = plt.subplots(\n",
    "        2, 1, \n",
    "        figsize=(14, 18), \n",
    "        gridspec_kw={'height_ratios': [3, 2]}, \n",
    "        constrained_layout=True\n",
    "    )\n",
    "\n",
    "    # Plot LC map\n",
    "    axes[0].imshow(rgba_image)\n",
    "    axes[0].set_title(title, fontsize=18, pad=15)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Plot histogram\n",
    "    bars = axes[1].barh(range(len(descriptions)), counts, color=bar_colors)\n",
    "    axes[1].set_xlabel(\"Number of Pixels\", fontsize=14)\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].set_title(\"Predicted classes distribution\", fontsize=16, pad=10)\n",
    "\n",
    "    # --- REMOVE Y TICKS and LABELS ---\n",
    "    axes[1].set_yticks([])  # remove ticks\n",
    "    axes[1].set_yticklabels([])  # remove labels\n",
    "\n",
    "    # --- Add class names INSIDE the bars ---\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        axes[1].text(\n",
    "            width * 0.01,              # Slightly after the start of the bar\n",
    "            bar.get_y() + bar.get_height() / 2,  # Vertical center of the bar\n",
    "            f\"  {descriptions[i]}\",\n",
    "            va='center', ha='left',\n",
    "            fontsize=12,\n",
    "            fontweight='bold',\n",
    "            color='white' if width > 15e6 else 'black'  # contrast\n",
    "        )\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved combined report to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_lc_map_with_histogram(lc_map_old, labels_df, title=\"21KUQ 2019 - LC map with existing pipeline\\n\", save_path=f\"lcmap_old.png\")\n",
    "plot_lc_map_with_histogram(lc_map_new_svmMc, labels_df, title=\"21KUQ 2019 - LC map with new pipeline - Multiclass SVM\\n\", save_path=f\"lcmap_new_svmMc.png\")\n",
    "plot_lc_map_with_histogram(lc_map_new_svmMcCal, labels_df, title=\"21KUQ 2019 - LC map with new pipeline - Multiclass SVM Calibrated\\n\", save_path=f\"lcmap_new_svmMcCal.png\")\n",
    "plot_lc_map_with_histogram(lc_map_new_svms, labels_df, title=\"21KUQ 2019 - LC map with existing pipeline - SVMs\\n\", save_path=f\"lcmap_new_svms.png\")\n",
    "plot_lc_map_with_histogram(lc_map_new_svmssoft, labels_df, title=\"21KUQ 2019 - LC map with new pipeline - SVMs with Softmax\\n\", save_path=f\"lcmap_new_svmssoft.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
