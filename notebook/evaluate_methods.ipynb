{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of cloud and shadow mask refinement methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from creds import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from rasterio.windows import from_bounds\n",
    "import rioxarray\n",
    "import tacoreader\n",
    "import rasterio as rio\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks refinement - Performance, time & resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_memory_usage(csv_path, note = ''):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    total_time = data[\"Elapsed Time (s)\"].iloc[-1]\n",
    "    avg_memory = data[\"Memory Usage (MB)\"].mean()\n",
    "    max_memory = data[\"Memory Usage (MB)\"].max()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data[\"Elapsed Time (s)\"], data[\"Memory Usage (MB)\"], label=\"Memory Usage\")\n",
    "    plt.title(f\"{note} - Memory Usage Over Time\\nAvg Time: {total_time:.2f}s (~{(total_time / 60):.2f}m) | Avg Memory: {avg_memory:.2f}MB | Max Memory: {max_memory:.2f}MB\")\n",
    "    plt.xlabel(\"Elapsed Time (s)\")\n",
    "    plt.ylabel(\"Memory Usage (MB)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def get_run_files(folder_path, id_prefix):\n",
    "\n",
    "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.startswith(id_prefix) and f.endswith('.csv')]\n",
    "\n",
    "def read_memory_data(file_paths):\n",
    "\n",
    "    data_frames = [pd.read_csv(file) for file in file_paths]\n",
    "    return data_frames\n",
    "\n",
    "def compute_stats(data_list):\n",
    "\n",
    "    avg_memory = np.mean([df[\"Memory Usage (MB)\"].mean() for df in data_list])\n",
    "    max_memory = max(df[\"Memory Usage (MB)\"].max() for df in data_list)\n",
    "    total_time = np.mean([df[\"Elapsed Time (s)\"].iloc[-1] for df in data_list])\n",
    "    return avg_memory, max_memory, total_time\n",
    "\n",
    "def plot_memory_usage_combined(id, new_folder, old_folder):\n",
    "\n",
    "    files_new = get_run_files(new_folder, id)\n",
    "    files_old = get_run_files(old_folder, id)\n",
    "    data_new = read_memory_data(files_new)\n",
    "    data_old = read_memory_data(files_old)\n",
    "    \n",
    "    avg_memory_new, max_memory_new, total_time_new = compute_stats(data_new)\n",
    "    avg_memory_old, max_memory_old, total_time_old = compute_stats(data_old)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 20), sharex=True)\n",
    "    \n",
    "    fig.text(0.5, 0.965, f\"Memory Usage Analysis - Tile {id}\", fontsize=20, ha='center')\n",
    "    fig.text(0.5, 0.945, f\"Comparison Between New and Existing Version\", fontsize=18, ha='center')\n",
    "    fig.text(0.5, 0.900, \\\n",
    "             f\"New Version: Avg Mem = {avg_memory_new:.2f}MB, Max Mem = {max_memory_new:.2f}MB, Average Time = {total_time_new:.2f}s\\n\"\n",
    "             f\"Existing Version: Avg Mem = {avg_memory_old:.2f}MB, Max Mem = {max_memory_old:.2f}MB, Average Time = {total_time_old:.2f}s\",\n",
    "             fontsize=16, ha='center')\n",
    "    \n",
    "    new_stats = \"\\n\".join(\n",
    "        [f\"Run {i+1}: Avg: {df['Memory Usage (MB)'].mean():.2f}MB | Max: {df['Memory Usage (MB)'].max():.2f}MB | Time: {df['Elapsed Time (s)'].iloc[-1]:.2f}s\" \n",
    "         for i, df in enumerate(data_new)]\n",
    "    )\n",
    "    for i, df in enumerate(data_new):\n",
    "        axes[0].plot(df[\"Elapsed Time (s)\"], df[\"Memory Usage (MB)\"], label=f\"Run {i+1}\")\n",
    "    axes[0].set_ylabel(\"Memory Usage (MB)\")\n",
    "    axes[0].set_title(f\"Memory Usage for {id} (New Version)\\n\\n{new_stats}\", fontsize=18)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid()\n",
    "    \n",
    "    old_stats = \"\\n\".join(\n",
    "        [f\"Run {i+1}: Avg: {df['Memory Usage (MB)'].mean():.2f}MB | Max: {df['Memory Usage (MB)'].max():.2f}MB | Time: {df['Elapsed Time (s)'].iloc[-1]:.2f}s\" \n",
    "         for i, df in enumerate(data_old)]\n",
    "    )\n",
    "    for i, df in enumerate(data_old):\n",
    "        axes[1].plot(df[\"Elapsed Time (s)\"], df[\"Memory Usage (MB)\"], label=f\"Run {i+1}\")\n",
    "    axes[1].set_xlabel(\"Elapsed Time (s)\")\n",
    "    axes[1].set_ylabel(\"Memory Usage (MB)\")\n",
    "    axes[1].set_title(f\"Memory Usage for {id} (Existing Version)\\n\\n{old_stats}\", fontsize=18, pad=15)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid()\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "    plt.savefig(\"lcmap_memory_usage.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single files inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_memory_usage('/media/datapart/lucazanolo/S2_processed_masks/logs/T18NWL_1.csv', note=\"Run 1 - Reimplemented Pipeline - Masks Refinemenet for tile 18NWL\")\n",
    "plot_memory_usage('/media/datapart/lucazanolo/S2_processed_masks/logs/T18NWL_2.csv', note = \"Run 2 - Reimplemented Pipeline - Masks Refinemenet for tile 18NWL\")\n",
    "plot_memory_usage('/media/datapart/lucazanolo/S2_processed_masks_old/logs/T18NWL_1.csv', note=\"Run 1 - Existing Pipeline - Masks Refinemenet for tile 18NWL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile processing report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"T10UED\"\n",
    "new_folder = \"/media/datapart/lucazanolo/S2_processed_masks/logs\"\n",
    "old_folder = \"/media/datapart/lucazanolo/S2_processed_masks_old/logs\"\n",
    "plot_memory_usage_combined(id, new_folder, old_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Cloud and Shadow masks with CloudSEN12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(config_map, title = None):\n",
    "    \"\"\" Structured print for a dict object\"\"\"\n",
    "    if title:\n",
    "        print(title)\n",
    "        \n",
    "    for i, (key, value) in enumerate(config_map.items(), 1):\n",
    "        print(f\"{i} - {key} : {value}\")\n",
    "        \n",
    "def print_dataframe(df, title='', columns_to_exclude=[]):\n",
    "    \"\"\" Structured print for a DataFrame object\"\"\"\n",
    "\n",
    "    if title:\n",
    "        print(title)\n",
    "    if df is not None and len(df) != 0:\n",
    "        df_to_show = df.drop(columns=columns_to_exclude)\n",
    "        print(tabulate(df_to_show, headers='keys', tablefmt='psql'))\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        \n",
    "def get_season(month):\n",
    "    \"\"\"\n",
    "    Return an id built with the first letter of the months of each season.\n",
    "    \"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'DJF'  # Winter (December, January, February)\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'MAM'  # Spring (March, April, May)\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'JJA'  # Summer (June, July, August)\n",
    "    else:\n",
    "        return 'SON'  # Fall (September, October, November)\n",
    "\n",
    "\n",
    "def load_cloussen12_metadata():\n",
    "    \n",
    "    # Load Cloudsen12 data and analyze for which seasons there 3 or more images of the same tile available -> keep the images in these groups\n",
    "    ds = tacoreader.load(\"tacofoundation:cloudsen12-l2a\")\n",
    "    ds['s2_date'] = pd.to_datetime(ds['s2_id'].str[11:19], format='%Y%m%d')\n",
    "\n",
    "    ds = ds[ds[\"label_type\"] == \"high\"]\n",
    "    ds = ds[ds[\"real_proj_shape\"] == 2000]\n",
    "\n",
    "    # Generating 4 new columns\n",
    "    # Used to extract MM-YYYY cloumn\n",
    "    ds[\"YearMonth\"] = pd.to_datetime(ds[\"s2_date\"]).dt.to_period(\"M\")\n",
    "\n",
    "    # Used to extract one of [DJF, MAM, JJA, SON] season ids.\n",
    "    ds[\"season\"] = ds[\"YearMonth\"].dt.month.apply(get_season)\n",
    "\n",
    "    # Create combined id to distinguish between seasons of different years\n",
    "    ds[\"YearSeason\"] = ds[\"season\"] + ds[\"YearMonth\"].dt.year.astype(str)\n",
    "\n",
    "    # Add a column with the explicit tile id\n",
    "    ds[\"tile\"] = ds[\"s2_id\"].str[39:44]\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def evaluate_with_classification_report(ground_truth, predicted_mask, label=\"Cloud\"):\n",
    "    print(ground_truth.shape, predicted_mask.shape)\n",
    "\n",
    "    y_true = ground_truth.flatten()[:2000*2000]\n",
    "    y_pred = predicted_mask.flatten()\n",
    "\n",
    "    print(\"Shapes:\", \"y_true:\", y_true.shape, \"y_pred:\", y_pred.shape)\n",
    "    # Ensure integer values\n",
    "    y_true = np.nan_to_num(y_true, nan=0).astype(int)\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0).astype(int)\n",
    "\n",
    "    unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    \n",
    "    if len(unique_labels) < 2:\n",
    "        print(f\"Warning: Not enough classes found in ground truth for {label} evaluation.\")\n",
    "        return \"Insufficient class variety for evaluation\"\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[f\"No {label}\", label], zero_division=0)\n",
    "\n",
    "    return report\n",
    "\n",
    "def crop_3d(array, target_shape=(2000, 2000)):\n",
    "\n",
    "    return array[:, :target_shape[0], :target_shape[1]]\n",
    "\n",
    "def crop_2d(array, target_shape=(2000, 2000)):\n",
    "\n",
    "    return array[:target_shape[0], :target_shape[1]]\n",
    "\n",
    "def read_cloudsen12_sample(dt, sample_idx):\n",
    "    s2 = dt.read(sample_idx).read(0)\n",
    "    s2_label = dt.read(sample_idx).read(1)\n",
    "\n",
    "    with rio.open(s2) as src, rio.open(s2_label) as dst:\n",
    "        s2_data = src.read([4, 3, 2])\n",
    "        s2_label_data = dst.read()\n",
    "        \n",
    "        s2_data = crop_3d(s2_data, target_shape=(2000, 2000))\n",
    "        s2_label_data = crop_3d(s2_label_data, target_shape=(2000, 2000))\n",
    "\n",
    "        patch_bounds = dst.bounds\n",
    "        crs = dst.crs\n",
    "\n",
    "    return s2_data, s2_label_data, patch_bounds, crs\n",
    "\n",
    "def read_patch_with_window(src_path, bounds, target_crs):\n",
    "    \"\"\"\n",
    "    Reads a raster patch using a window defined by bounds.\n",
    "    \"\"\"\n",
    "    with rio.open(src_path) as src:\n",
    "        if src.crs != target_crs:\n",
    "            raise ValueError(f\"CRS mismatch: Source CRS is {src.crs}, but expected {target_crs}\")\n",
    "        window = from_bounds(*bounds, transform=src.transform)\n",
    "        patch = src.read(1, window=window)\n",
    "    return patch\n",
    "\n",
    "def evaluation(paths : dict, tile):\n",
    "    \n",
    "    ds = load_cloussen12_metadata()\n",
    "    filtered_subset = ds.groupby([\"tile\", \"YearSeason\"]) \\\n",
    "        .filter(lambda x: len(x) >= 3) \\\n",
    "        .sort_values(\"tile\")\n",
    "    filtered_subset = filtered_subset[filtered_subset['tile'] == tile]\n",
    "\n",
    "    #sen2cor_cloud_src = rioxarray.open_rasterio(paths.values()[0]).squeeze()\n",
    "    #refined_cloud_src = rioxarray.open_rasterio(paths.values()[1]).squeeze()\n",
    "    #sen2cor_shadow_src = rioxarray.open_rasterio(paths.values()[2]).squeeze()\n",
    "    #refined_shadow_src = rioxarray.open_rasterio(paths.values()[3]).squeeze()\n",
    "\n",
    "    print_dataframe(filtered_subset, f\"Filtered subset of samples -> {len(filtered_subset)} samples\")\n",
    "    print(\"Spatial information:\\n\")\n",
    "    #print_dict(refined_cloud_src.spatial_ref.attrs)\n",
    "\n",
    "    all_cloud_gt = []\n",
    "    all_sen2cor_cloud_pred = []\n",
    "    all_refined_cloud_pred = []\n",
    "    all_shadow_gt = []\n",
    "    all_sen2cor_shadow_pred = []\n",
    "    all_refined_shadow_pred = []\n",
    "\n",
    "    verbose = False\n",
    "    \n",
    "    for i in range(len(filtered_subset)):\n",
    "        \n",
    "        print(f\"\\n\\nEvaluating for patch {i}:\")\n",
    "        print(f\" - Downloading data points from CloudSen12 ...\")\n",
    "        datapoint, datapoint_gt, patch_bounds, crs = read_cloudsen12_sample(filtered_subset, sample_idx = i)\n",
    "        datapoint_gt = datapoint_gt.squeeze()\n",
    "        info = filtered_subset.iloc[0]\n",
    "\n",
    "        cloud_gt = np.where((datapoint_gt == 1) | (datapoint_gt == 2), 1, 0)\n",
    "        shadow_gt = np.where(datapoint_gt == 3, 1, 0)\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            print_dataframe(pd.DataFrame(info), title = \" - Current datapoint info\")\n",
    "        \n",
    "        refined_cloud_patch = read_patch_with_window(list(paths.values())[0], patch_bounds, crs)\n",
    "        refined_shadow_patch = read_patch_with_window(list(paths.values())[1], patch_bounds, crs)\n",
    "        sen2cor_cloud_patch = read_patch_with_window(list(paths.values())[2], patch_bounds, crs)\n",
    "        sen2cor_shadow_patch = read_patch_with_window(list(paths.values())[3], patch_bounds, crs)\n",
    "\n",
    "        print(\" - Datapoints downloaded, shape: \", datapoint.shape)\n",
    "        print(\" - Datapoints gt downloaded, shape: \", datapoint_gt.shape)\n",
    "        print(\" - sen2cor_cloud_patch, shape: \", sen2cor_cloud_patch.shape)\n",
    "        print(\" - refined_cloud_patch, shape: \", refined_cloud_patch.shape)\n",
    "        print(\" - sen2cor_shadow_patch, shape: \", sen2cor_shadow_patch.shape)\n",
    "        print(\" - refined_shadow_patch, shape: \", refined_shadow_patch.shape)\n",
    "        \n",
    "        # Define minimal common shape for alignment\n",
    "        min_x = min(sen2cor_cloud_patch.shape[0], datapoint_gt.shape[0])\n",
    "        min_y = min(sen2cor_cloud_patch.shape[1], datapoint_gt.shape[1])\n",
    "        final_shape = (min_x, min_y)\n",
    "\n",
    "        # Top-left crop to align all patches\n",
    "        cloud_gt = crop_2d(cloud_gt, final_shape)\n",
    "        shadow_gt = crop_2d(shadow_gt, final_shape)\n",
    "        sen2cor_cloud_patch = crop_2d(sen2cor_cloud_patch, final_shape)\n",
    "        refined_cloud_patch = crop_2d(refined_cloud_patch, final_shape)\n",
    "        sen2cor_shadow_patch = crop_2d(sen2cor_shadow_patch, final_shape)\n",
    "        refined_shadow_patch = crop_2d(refined_shadow_patch, final_shape)\n",
    "\n",
    "        print(f\" - Final cropped shape for all patches: {final_shape}\")\n",
    "        print(\" - Datapoint classes distribution: \", np.unique(datapoint_gt, return_counts=True))\n",
    "        print(\" - Cloud ground truth: \", np.unique(cloud_gt, return_counts=True))\n",
    "        print(\" - Shadow ground truth: \", np.unique(shadow_gt, return_counts=True))\n",
    "        print(f\" - Unique values in Sen2Cor cloud mask patch: {np.unique(sen2cor_cloud_patch, return_counts=True)}\")\n",
    "        print(f\" - Unique values in Refined cloud mask patch: {np.unique(refined_cloud_patch, return_counts=True)}\")\n",
    "        print(f\" - Unique values in Sen2Cor shadow mask patch: {np.unique(sen2cor_shadow_patch, return_counts=True)}\")\n",
    "        print(f\" - Unique values in Refined shadow mask patch: {np.unique(refined_shadow_patch, return_counts=True)}\")\n",
    "\n",
    "        all_cloud_gt.extend(cloud_gt.flatten())\n",
    "        all_sen2cor_cloud_pred.extend(sen2cor_cloud_patch.flatten())\n",
    "        all_refined_cloud_pred.extend(refined_cloud_patch.flatten())\n",
    "\n",
    "        all_shadow_gt.extend(shadow_gt.flatten())\n",
    "        all_sen2cor_shadow_pred.extend(sen2cor_shadow_patch.flatten())\n",
    "        all_refined_shadow_pred.extend(refined_shadow_patch.flatten())\n",
    "        \n",
    "    print(\"\\nSen2Cor Cloud Mask Evaluation:\\n\", classification_report(all_cloud_gt, all_sen2cor_cloud_pred, zero_division=0, target_names=['No Cloud','Cloud']))\n",
    "    print(\"\\nRefined Cloud Mask Evaluation:\\n\", classification_report(all_cloud_gt, all_refined_cloud_pred, zero_division=0, target_names=['No Cloud','Cloud']))\n",
    "    print(\"\\nSen2Cor Shadow Mask Evaluation:\\n\", classification_report(all_shadow_gt, all_sen2cor_shadow_pred, zero_division=0, target_names=['No Shadow','Shadow']))\n",
    "    print(\"\\nRefined Shadow Mask Evaluation:\\n\", classification_report(all_shadow_gt, all_refined_shadow_pred, zero_division=0,target_names=['No Shadow','Shadow'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CloudSen12 - Inspect and filter samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cloudsen12_metadata = load_cloussen12_metadata()\n",
    "# Look how many data are available for each YearSeason in CloudSen12 for training\n",
    "# Filtered to keep data of tiles with more than 3 samples.\n",
    "\n",
    "GROUP_MIN = 4\n",
    "TILE = \"10UED\"\n",
    "\n",
    "filtered_subset = cloudsen12_metadata.groupby([\"tile\", \"YearSeason\"]).filter(lambda x: len(x) >= GROUP_MIN).sort_values(\"tile\")[cloudsen12_metadata['tile'] == TILE]\n",
    "print_dataframe(filtered_subset, f\"Filtered subset of samples -> {len(filtered_subset)} samples\")\n",
    "\n",
    "# Dataframe not filtered grouped by tile, to understand how many samples are available for each tile\n",
    "\n",
    "grouped_counts = cloudsen12_metadata.groupby([\"tile\", \"YearSeason\"]).size().reset_index(name=\"image_count\")\n",
    "print_dataframe(grouped_counts[grouped_counts[\"image_count\"] >= GROUP_MIN], f\"Retained groups of {GROUP_MIN} or more elements of the same tile per season\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CloudSEN12 - Inspect patch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image from the test dataset\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap\n",
    "idx = 1\n",
    "subset = filtered_subset.iloc[idx]\n",
    "datapoint, datapoint_gt, patch_bounds, crs = read_cloudsen12_sample(filtered_subset, sample_idx = idx)\n",
    "datapoint_gt = datapoint_gt.squeeze()\n",
    "print(datapoint_gt.shape, datapoint.shape)\n",
    "\n",
    "# RGB normalization\n",
    "datapoint_rgb = datapoint.transpose(1, 2, 0) / 3000\n",
    "\n",
    "# Colormap personalizzata per le classi 0-3\n",
    "cloudsen_cmap = ListedColormap(['skyblue', 'white', 'lightgray', 'black'])\n",
    "\n",
    "# Legenda corrispondente\n",
    "legend_gt = [\n",
    "    Patch(color='skyblue', label='0 - Clear'),\n",
    "    Patch(color='white', label='1 - Thick Cloud'),\n",
    "    Patch(color='lightgray', label='2 - Thin Cloud'),\n",
    "    Patch(color='black', label='3 - Cloud Shadow'),\n",
    "]\n",
    "\n",
    "# Colormap binaria e legenda per le maschere\n",
    "legend_binary1 = [\n",
    "    Patch(color='black', label='No Cloud'),\n",
    "    Patch(color='white', label='Cloud Pixel'),\n",
    "]\n",
    "\n",
    "legend_binary2 = [\n",
    "    Patch(color='black', label='No Shadow'),\n",
    "    Patch(color='white', label='Shadow Pixel'),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# RGB patch (no legend)\n",
    "ax[0, 0].imshow(datapoint_rgb)\n",
    "ax[0, 0].set_title(\"RGB patch\")\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "# Human annotated patch\n",
    "ax[0, 1].imshow(datapoint_gt, cmap=cloudsen_cmap, vmin=0, vmax=3)\n",
    "ax[0, 1].set_title(\"Human annotated patch\")\n",
    "ax[0, 1].axis('off')\n",
    "ax[0, 1].legend(handles=legend_gt, loc='upper right', fontsize=8, frameon=True)\n",
    "\n",
    "# Cloud mask (1 or 2 → 1)\n",
    "cloud_mask = np.where((datapoint_gt == 1) | (datapoint_gt == 2), 1, 0)\n",
    "ax[1, 0].imshow(cloud_mask, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[1, 0].set_title(\"Cloud mask\")\n",
    "ax[1, 0].axis('off')\n",
    "ax[1, 0].legend(handles=legend_binary1, loc='upper right', fontsize=8, frameon=True)\n",
    "\n",
    "# Shadow mask (3 → 1)\n",
    "shadow_mask = np.where(datapoint_gt == 3, 1, 0)\n",
    "ax[1, 1].imshow(shadow_mask, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[1, 1].set_title(\"Shadow mask\")\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].legend(handles=legend_binary2, loc='upper right', fontsize=8, frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download UnetmobV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the model\n",
    "model_path = \"https://huggingface.co/datasets/isp-uv-es/CloudSEN12Plus/resolve/main/demo/models/UNetMobV2_V2.pt\"\n",
    "local_model_path = \"../models/UNetMobV2_V2.pt\"\n",
    "with requests.get(model_path, stream=True) as r:\n",
    "    with open(local_model_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# Load the weights into the model\n",
    "model_v2 = smp.Unet(encoder_name=\"mobilenet_v2\", encoder_weights=None, classes=4, in_channels=13)\n",
    "model_v2.load_state_dict(torch.load(local_model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Desactivate the gradient estimation\n",
    "for param in model_v2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_v2 = model_v2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T18NWL - Existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t18nwl_sen2cor_masks = {\n",
    "    \"sen2cor_cloud_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks_old/cloud_masks/MSIL2A_20190223T152641_N0500_R025_T18NWL_20221213T203925_cloudMediumMask_Sen2Cor.tif\",\n",
    "    \"sen2cor_shadow_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks_old/shadow_masks/MSIL2A_20190223T152641_N0500_R025_T18NWL_20221213T203925_shadowMask_Sen2Cor.tif\"\n",
    "}\n",
    "t18nwl_old = {\n",
    "    \"refined_cloud_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks_old/cloud_masks/MSIL2A_20190223T152641_N0500_R025_T18NWL_20221213T203925_cloudMediumMask.tif\",\n",
    "    \"refined_shadow_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks_old/shadow_masks/MSIL2A_20190223T152641_N0500_R025_T18NWL_20221213T203925_shadowMask.tif\"\n",
    "}\n",
    "t18nwl_old.update(t18nwl_sen2cor_masks)\n",
    "print_dict(t18nwl_old, title=\"Specified paths:\")\n",
    "evaluation(t18nwl_old, tile='18NWL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Refined Cloud Mask Evaluation:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "    No Cloud       0.79      0.29      0.43   8610243\n",
    "       Cloud       0.51      0.90      0.65   7097757\n",
    "\n",
    "    accuracy                           0.57  15708000\n",
    "   macro avg       0.65      0.60      0.54  15708000\n",
    "weighted avg       0.66      0.57      0.53  15708000\n",
    "\n",
    "Refined Cloud Mask Evaluation:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "    No Cloud       0.83      0.38      0.52   8610243\n",
    "       Cloud       0.55      0.91      0.68   7097757\n",
    "\n",
    "    accuracy                           0.62  15708000\n",
    "   macro avg       0.69      0.64      0.60  15708000\n",
    "weighted avg       0.70      0.62      0.59  15708000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T18NWL - Reimplemented pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t18nwl_new = {\n",
    "    \"refined_cloud_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks/cloud_masks/MSIL2A_20190223T152641_N0500_R025_T18NWL_20221213T203925_cloudMediumMask.tif\",\n",
    "    \"refined_shadow_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks/shadow_masks/MSIL2A_20190223T152641_N0500_R025_T18NWL_20221213T203925_shadowMask.tif\"\n",
    "}\n",
    "t18nwl_new.update(t18nwl_sen2cor_masks)\n",
    "print_dict(t18nwl_new, title=\"Specified paths:\")\n",
    "evaluation(t18nwl_new, tile = \"18NWL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T10UED - Existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t10ued_sen2cor_masks = {\n",
    "    \"sen2cor_cloud_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks_old/cloud_masks/MSIL2A_20191006T192251_N0500_R099_T10UED_20230702T041139_cloudMediumMask_Sen2Cor.tif\",\n",
    "    \"sen2cor_shadow_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks_old/shadow_masks/MSIL2A_20191006T192251_N0500_R099_T10UED_20230702T041139_shadowMask_Sen2Cor.tif\"\n",
    "}\n",
    "t10ued_old = {\n",
    "    \"refined_cloud_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks_old/cloud_masks/MSIL2A_20191006T192251_N0500_R099_T10UED_20230702T041139_cloudMediumMask.tif\",\n",
    "    \"refined_shadow_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks_old/shadow_masks/MSIL2A_20191006T192251_N0500_R099_T10UED_20230702T041139_shadowMask.tif\"\n",
    "}\n",
    "t10ued_old.update(t10ued_sen2cor_masks)\n",
    "print_dict(t10ued_old, title=\"Specified paths:\")\n",
    "evaluation(t10ued_old, tile = \"10UED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T10UED - Reimplemented pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t10ued_new = {\n",
    "    \"refined_cloud_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks/cloud_masks/MSIL2A_20191006T192251_N0500_R099_T10UED_20230702T041139_cloudMediumMask.tif\",\n",
    "    \"refined_shadow_mask\" : \"/media/datapart/lucazanolo/S2_processed_masks/shadow_masks/MSIL2A_20191006T192251_N0500_R099_T10UED_20230702T041139_shadowMask.tif\",\n",
    "}\n",
    "t10ued_new.update(t10ued_sen2cor_masks)\n",
    "print_dict(t10ued_new, title=\"Specified paths:\")\n",
    "evaluation(t10ued_new, tile = \"10UED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LC maps generation - Performance, time & resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_memory_usage_combined(id, new_folder, old_folder):\n",
    "\n",
    "    files_new = get_run_files(new_folder, id)\n",
    "    files_old = get_run_files(old_folder, '1map')\n",
    "    data_new = read_memory_data(files_new)\n",
    "    data_old = read_memory_data(files_old)\n",
    "    \n",
    "    avg_memory_new, max_memory_new, total_time_new = compute_stats(data_new)\n",
    "    avg_memory_old, max_memory_old, total_time_old = compute_stats(data_old)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 20), sharex=True)\n",
    "    \n",
    "    fig.text(0.5, 0.965, f\"Memory Usage Analysis - Land Cover map generation\", fontsize=20, ha='center')\n",
    "    fig.text(0.5, 0.945, f\"Comparison Between New and Existing Version\", fontsize=18, ha='center')\n",
    "    fig.text(0.5, 0.900, \\\n",
    "             f\"New Version: Avg Mem = {avg_memory_new:.2f}MB, Max Mem = {max_memory_new:.2f}MB, Average Time = {total_time_new:.2f}s\\n\"\n",
    "             f\"Existing Version: Avg Mem = {avg_memory_old:.2f}MB, Max Mem = {max_memory_old:.2f}MB, Average Time = {total_time_old:.2f}s\",\n",
    "             fontsize=16, ha='center')\n",
    "    \n",
    "    new_stats = \"\\n\".join(\n",
    "        [f\"Run {i+1}: Avg: {df['Memory Usage (MB)'].mean():.2f}MB | Max: {df['Memory Usage (MB)'].max():.2f}MB | Time: {df['Elapsed Time (s)'].iloc[-1]:.2f}s\" \n",
    "         for i, df in enumerate(data_new)]\n",
    "    )\n",
    "    for i, df in enumerate(data_new):\n",
    "        axes[0].plot(df[\"Elapsed Time (s)\"], df[\"Memory Usage (MB)\"], label=f\"Run {i+1}\")\n",
    "    axes[0].set_ylabel(\"Memory Usage (MB)\")\n",
    "    axes[0].set_title(f\"Memory Usage for {id} (New Version)\\n\\n{new_stats}\", fontsize=18)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid()\n",
    "    \n",
    "    old_stats = \"\\n\".join(\n",
    "        [f\"Run {i+1}: Avg: {df['Memory Usage (MB)'].mean():.2f}MB | Max: {df['Memory Usage (MB)'].max():.2f}MB | Time: {df['Elapsed Time (s)'].iloc[-1]:.2f}s\" \n",
    "         for i, df in enumerate(data_old)]\n",
    "    )\n",
    "    for i, df in enumerate(data_old):\n",
    "        axes[1].plot(df[\"Elapsed Time (s)\"], df[\"Memory Usage (MB)\"], label=f\"Run {i+1}\")\n",
    "    axes[1].set_xlabel(\"Elapsed Time (s)\")\n",
    "    axes[1].set_ylabel(\"Memory Usage (MB)\")\n",
    "    axes[1].set_title(f\"Memory Usage for {id} (Existing Version)\\n\\n{old_stats}\", fontsize=18, pad=15)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid()\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.88])\n",
    "    plt.savefig(\"lcmap_memory_usage.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_memory_usage(csv_path, note = ''):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    total_time = data[\"Elapsed Time (s)\"].iloc[-1]\n",
    "    avg_memory = data[\"Memory Usage (MB)\"].mean()\n",
    "    max_memory = data[\"Memory Usage (MB)\"].max()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data[\"Elapsed Time (s)\"], data[\"Memory Usage (MB)\"], label=\"Memory Usage\")\n",
    "    plt.title(f\"{note} - Memory Usage Over Time\\nTime: {total_time:.2f}s (~{(total_time / 60):.2f}m) | Avg Memory: {avg_memory:.2f}MB | Max Memory: {max_memory:.2f}MB\")\n",
    "    plt.xlabel(\"Elapsed Time (s)\")\n",
    "    plt.ylabel(\"Memory Usage (MB)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_memory_usage('/media/datapart/lucazanolo/SVM_old/classification/logs/cloud-detection_2025-04-23-07-59.csv', note=\"Existing Pipeline - LC map generation\")\n",
    "id = \"4map\"\n",
    "new_folder = \"/media/datapart/lucazanolo/SVM/lc_maps/logs\"\n",
    "old_folder = \"/media/datapart/lucazanolo/SVM_old/classification/logs\"\n",
    "plot_memory_usage_combined(id, new_folder, old_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
